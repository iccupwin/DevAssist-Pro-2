"""
КП Анализатор v3 - Основная бизнес-логика
Экспертная система с 10-критериальным анализом
"""
import asyncio
import json
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional
from fastapi import HTTPException, UploadFile
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session
from io import BytesIO
from pathlib import Path

from ..shared.models import User, Document, V3Analysis, V3AnalysisDocument
from ..services.llm.orchestrator import LLMOrchestrator
from .documents.core.v3_document_processor import V3DocumentProcessor
from .reports.core.kp_pdf_exporter import KPAnalysisPDFExporter
from ..api.v3.schemas import V3AnalysisRequest, CriteriaWeight

logger = logging.getLogger(__name__)

class V3AnalyzerService:
    """Сервис для v3 анализа коммерческих предложений"""
    
    def __init__(self, db: Session):
        self.db = db
        self.llm_orchestrator = LLMOrchestrator()
        self.document_processor = V3DocumentProcessor()
        self.pdf_exporter = KPAnalysisPDFExporter()
        
    async def upload_and_process_document(
        self, 
        file: UploadFile, 
        user: User
    ) -> Document:
        """Загрузка и обработка документа с расширенной экстракцией"""
        try:
            # Читаем файл
            file_content = await file.read()
            
            # Сохраняем файл на диск
            file_path = await self._save_uploaded_file(file, file_content)
            
            # Извлекаем контент с расширенными возможностями
            extraction_result = await self.document_processor.extract_advanced_content(
                file_content, file.filename
            )
            
            # Создаем запись документа в БД
            document = Document(
                filename=self._generate_unique_filename(file.filename),
                original_filename=file.filename,
                file_size=len(file_content),
                file_type=self._get_file_type(file.filename),
                document_type="kp",  # По умолчанию КП
                file_path=str(file_path),
                is_processed=True,
                processed_at=datetime.utcnow(),
                processing_status="completed",
                extracted_text=extraction_result["text"],
                document_metadata={
                    "tables_count": len(extraction_result["tables"]),
                    "currencies_count": len(extraction_result["currencies"]),
                    "extraction_method": "v3_advanced",
                    "structured_data": extraction_result["structured_data"]
                },
                uploaded_by_id=user.id
            )
            
            self.db.add(document)
            self.db.commit()
            self.db.refresh(document)
            
            logger.info(f"✅ Document processed: {file.filename} -> ID {document.id}")
            return document
            
        except Exception as e:
            logger.error(f"❌ Document processing error: {e}")
            self.db.rollback()
            raise HTTPException(status_code=500, detail=f"Document processing failed: {str(e)}")
    
    async def perform_expert_analysis(
        self,
        request: V3AnalysisRequest,
        user: User
    ) -> V3Analysis:
        """Выполнить экспертный анализ по 10 критериям"""
        start_time = datetime.utcnow()
        
        try:
            # Проверяем существование документов
            documents = self._validate_documents(request.document_ids, user.id)
            
            # Получаем ТЗ если указано
            tz_document = None
            if request.tz_document_id:
                tz_document = self._get_tz_document(request.tz_document_id, user.id)
            
            # Определяем веса критериев
            weights = self._resolve_criteria_weights(request.analysis_config)
            
            # Комбинируем контент документов
            combined_content = await self._combine_documents_content(documents)
            
            # Получаем контент ТЗ для сравнения
            tz_content = tz_document.extracted_text if tz_document else None
            
            # Выполняем AI анализ
            analysis_result = await self._perform_ai_analysis(
                combined_content, weights, tz_content
            )
            
            # Генерируем диаграммы если запрошено
            charts_data = None
            if request.generate_charts:
                charts_data = await self._generate_analysis_charts(analysis_result)
            
            # Рассчитываем время обработки
            processing_time = (datetime.utcnow() - start_time).total_seconds()\n            \n            # Создаем запись анализа\n            analysis = V3Analysis(\n                overall_score=analysis_result.get(\"overall_score\", 0),\n                weighted_score=analysis_result.get(\"weighted_score\", 0.0),\n                company_name=analysis_result.get(\"company_name\", \"Unknown\"),\n                executive_summary=analysis_result.get(\"executive_summary\", \"\"),\n                criteria_results=analysis_result.get(\"business_analysis\", {}),\n                criteria_weights=weights.dict(),\n                currency_data=combined_content.get(\"currencies_summary\", {}),\n                extracted_tables=combined_content.get(\"tables_summary\", []),\n                charts_data=charts_data,\n                processing_time=processing_time,\n                risk_level=analysis_result.get(\"risk_level\", \"Умеренный\"),\n                recommendations=analysis_result.get(\"recommendations\", []),\n                status=\"completed\",\n                tz_document_id=request.tz_document_id,\n                created_by_id=user.id,\n                project_id=1  # TODO: получать из контекста или запроса\n            )\n            \n            self.db.add(analysis)\n            self.db.flush()  # Получаем ID\n            \n            # Связываем анализ с документами\n            for doc in documents:\n                analysis_doc = V3AnalysisDocument(\n                    v3_analysis_id=analysis.id,\n                    document_id=doc.id,\n                    extraction_data=doc.document_metadata\n                )\n                self.db.add(analysis_doc)\n            \n            self.db.commit()\n            self.db.refresh(analysis)\n            \n            logger.info(f\"✅ V3 Analysis completed: ID {analysis.id}, Score: {analysis.overall_score}\")\n            return analysis\n            \n        except Exception as e:\n            logger.error(f\"❌ V3 Analysis error: {e}\")\n            self.db.rollback()\n            raise HTTPException(status_code=500, detail=f\"Analysis failed: {str(e)}\")\n    \n    async def generate_professional_pdf(\n        self, \n        analysis_id: int, \n        user: User\n    ) -> StreamingResponse:\n        \"\"\"Генерация профессионального PDF отчета\"\"\"\n        try:\n            # Получаем анализ\n            analysis = self.db.query(V3Analysis).filter(\n                V3Analysis.id == analysis_id,\n                V3Analysis.created_by_id == user.id\n            ).first()\n            \n            if not analysis:\n                raise HTTPException(status_code=404, detail=\"Analysis not found\")\n            \n            # Генерируем PDF\n            pdf_buffer = await self.pdf_exporter.generate_v3_analysis_pdf(analysis)\n            \n            # Создаем имя файла\n            company_name = analysis.company_name or \"Unknown_Company\"\n            safe_name = company_name.replace(\" \", \"_\")[:50]\n            filename = f\"KP_Analysis_V3_{safe_name}_{analysis.id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf\"\n            \n            # Сохраняем в папку отчетов\n            reports_dir = Path(\"data/reports\")\n            reports_dir.mkdir(parents=True, exist_ok=True)\n            \n            pdf_path = reports_dir / filename\n            with open(pdf_path, 'wb') as f:\n                f.write(pdf_buffer.read())\n            \n            # Возвращаем как streaming response\n            pdf_buffer.seek(0)\n            return StreamingResponse(\n                BytesIO(pdf_buffer.read()),\n                media_type=\"application/pdf\",\n                headers={\"Content-Disposition\": f\"attachment; filename={filename}\"}\n            )\n            \n        except Exception as e:\n            logger.error(f\"❌ PDF generation error: {e}\")\n            raise HTTPException(status_code=500, detail=f\"PDF generation failed: {str(e)}\")\n    \n    def _validate_documents(self, document_ids: List[int], user_id: int) -> List[Document]:\n        \"\"\"Проверка существования и доступности документов\"\"\"\n        documents = self.db.query(Document).filter(\n            Document.id.in_(document_ids),\n            Document.uploaded_by_id == user_id\n        ).all()\n        \n        if len(documents) != len(document_ids):\n            missing_ids = set(document_ids) - {doc.id for doc in documents}\n            raise HTTPException(\n                status_code=404, \n                detail=f\"Documents not found: {list(missing_ids)}\"\n            )\n        \n        return documents\n    \n    def _get_tz_document(self, tz_id: int, user_id: int) -> Document:\n        \"\"\"Получение документа ТЗ\"\"\"\n        tz_doc = self.db.query(Document).filter(\n            Document.id == tz_id,\n            Document.uploaded_by_id == user_id\n        ).first()\n        \n        if not tz_doc:\n            raise HTTPException(status_code=404, detail=\"TZ document not found\")\n        \n        return tz_doc\n    \n    def _resolve_criteria_weights(self, config: Optional[Any]) -> CriteriaWeight:\n        \"\"\"Определение весов критериев\"\"\"\n        if config and config.custom_weights:\n            return config.custom_weights\n        elif config and config.preset:\n            return self._get_preset_weights(config.preset)\n        else:\n            return CriteriaWeight()  # Default balanced\n    \n    def _get_preset_weights(self, preset_name: str) -> CriteriaWeight:\n        \"\"\"Получение предустановленных весов\"\"\"\n        presets = {\n            \"balanced\": CriteriaWeight(),\n            \"budget_focused\": CriteriaWeight(\n                budget_compliance=0.25, timeline_compliance=0.15,\n                technical_compliance=0.15, functional_coverage=0.12\n            ),\n            \"technical_focused\": CriteriaWeight(\n                technical_compliance=0.25, functional_coverage=0.20,\n                team_expertise=0.15, budget_compliance=0.10\n            )\n        }\n        \n        return presets.get(preset_name, CriteriaWeight())\n    \n    async def _combine_documents_content(self, documents: List[Document]) -> Dict[str, Any]:\n        \"\"\"Объединение контента документов\"\"\"\n        combined = {\n            \"text\": \"\",\n            \"tables\": [],\n            \"currencies\": [],\n            \"tables_summary\": [],\n            \"currencies_summary\": {}\n        }\n        \n        for doc in documents:\n            # Текст\n            combined[\"text\"] += doc.extracted_text or \"\"\n            combined[\"text\"] += \"\\n\\n\"\n            \n            # Метаданные\n            metadata = doc.document_metadata or {}\n            \n            # Таблицы (из метаданных)\n            if \"tables\" in metadata:\n                combined[\"tables\"].extend(metadata[\"tables\"])\n            \n            # Валюты (из метаданных)  \n            if \"currencies\" in metadata:\n                combined[\"currencies\"].extend(metadata[\"currencies\"])\n        \n        # Суммируем для отчета\n        combined[\"tables_summary\"] = combined[\"tables\"][:5]  # Первые 5 таблиц\n        \n        # Группируем валюты\n        currency_groups = {}\n        for curr in combined[\"currencies\"]:\n            curr_type = curr.get(\"currency\", \"unknown\")\n            if curr_type not in currency_groups:\n                currency_groups[curr_type] = {\"count\": 0, \"total\": 0}\n            currency_groups[curr_type][\"count\"] += 1\n            currency_groups[curr_type][\"total\"] += curr.get(\"amount\", 0)\n        \n        combined[\"currencies_summary\"] = currency_groups\n        \n        return combined\n    \n    async def _perform_ai_analysis(\n        self, \n        content_data: Dict[str, Any], \n        weights: CriteriaWeight,\n        tz_content: Optional[str] = None\n    ) -> Dict[str, Any]:\n        \"\"\"Выполнение AI анализа через LLM оркестратор\"\"\"\n        try:\n            # Формируем промпт для v3 анализа\n            analysis_prompt = self._build_v3_analysis_prompt(weights, tz_content)\n            \n            # Подготавливаем контент\n            main_content = content_data[\"text\"]\n            tables_info = self._format_tables_for_prompt(content_data.get(\"tables\", []))\n            currencies_info = self._format_currencies_for_prompt(content_data.get(\"currencies_summary\", {}))\n            \n            full_content = f\"{main_content}\\n\\n{tables_info}\\n\\n{currencies_info}\"\n            \n            # Вызываем LLM через оркестратор\n            result = await self.llm_orchestrator.analyze_content(\n                prompt=analysis_prompt,\n                content=full_content[:8000],  # Ограничиваем длину\n                model=\"claude-3-haiku-20240307\",\n                max_tokens=4000\n            )\n            \n            # Парсим результат или используем fallback\n            if isinstance(result, dict):\n                return result\n            else:\n                return self._parse_llm_response(result, weights)\n                \n        except Exception as e:\n            logger.warning(f\"⚠️ LLM analysis failed: {e}, using fallback\")\n            return self._generate_fallback_analysis(content_data, weights)\n    \n    def _build_v3_analysis_prompt(self, weights: CriteriaWeight, tz_content: Optional[str]) -> str:\n        \"\"\"Построение промпта для v3 анализа\"\"\"\n        prompt = f\"\"\"\nВы - эксперт-аналитик по оценке коммерческих предложений в сфере IT и разработки программного обеспечения.\n\nЗАДАЧА: Проведите детальный анализ коммерческого предложения по 10 критериям с весовыми коэффициентами.\n\nКРИТЕРИИ И ВЕСА:\n1. Соответствие бюджету ({weights.budget_compliance:.1%})\n2. Соответствие срокам ({weights.timeline_compliance:.1%})\n3. Техническое соответствие ({weights.technical_compliance:.1%})\n4. Экспертиза команды ({weights.team_expertise:.1%})\n5. Функциональное покрытие ({weights.functional_coverage:.1%})\n6. Обеспечение качества ({weights.quality_assurance:.1%})\n7. Методология разработки ({weights.development_methodology:.1%})\n8. Масштабируемость ({weights.scalability:.1%})\n9. Коммуникация ({weights.communication:.1%})\n10. Добавленная стоимость ({weights.added_value:.1%})\n\nФОРМАТ ОТВЕТА - строго JSON:\n{{\n    \"company_name\": \"название компании\",\n    \"overall_score\": общая_оценка_0_100,\n    \"weighted_score\": взвешенная_оценка_с_учетом_весов,\n    \"executive_summary\": \"краткое резюме анализа\",\n    \"business_analysis\": {{\n        \"budget_compliance\": {{\"score\": 0-100, \"weight\": {weights.budget_compliance}, \"details\": \"описание\", \"recommendations\": [\"рек\"], \"compliance_level\": \"high/medium/low\", \"risk_factors\": [\"риск\"]}}\n        // ... остальные критерии\n    }},\n    \"recommendations\": [\"рекомендация 1\", \"рекомендация 2\"],\n    \"risk_level\": \"Низкий/Умеренный/Высокий\"\n}}\n        \"\"\"\n        \n        if tz_content:\n            prompt += f\"\\n\\nТЕХНИЧЕСКОЕ ЗАДАНИЕ ДЛЯ СРАВНЕНИЯ:\\n{tz_content[:1000]}\\n\"\n        \n        return prompt\n    \n    def _format_tables_for_prompt(self, tables: List[Dict]) -> str:\n        \"\"\"Форматирование таблиц для промпта\"\"\"\n        if not tables:\n            return \"\"\n        \n        result = \"\\nИЗВЛЕЧЕННЫЕ ТАБЛИЦЫ:\\n\"\n        for i, table in enumerate(tables[:3]):  # Только первые 3\n            result += f\"Таблица {i+1}:\\n\"\n            data = table.get(\"data\", [])\n            for row in data[:5]:  # Только первые 5 строк\n                result += \" | \".join([str(cell)[:30] for cell in row]) + \"\\n\"\n            result += \"\\n\"\n        \n        return result\n    \n    def _format_currencies_for_prompt(self, currencies: Dict[str, Any]) -> str:\n        \"\"\"Форматирование валют для промпта\"\"\"\n        if not currencies:\n            return \"\"\n        \n        result = \"\\nОБНАРУЖЕННЫЕ ВАЛЮТЫ:\\n\"\n        for curr_type, data in currencies.items():\n            result += f\"{curr_type}: {data.get('count', 0)} упоминаний, сумма: {data.get('total', 0):.2f}\\n\"\n        \n        return result\n    \n    def _parse_llm_response(self, response: str, weights: CriteriaWeight) -> Dict[str, Any]:\n        \"\"\"Парсинг ответа LLM\"\"\"\n        try:\n            return json.loads(response)\n        except json.JSONDecodeError:\n            logger.warning(\"Failed to parse LLM JSON response, using fallback\")\n            return self._generate_fallback_analysis({\"text\": response}, weights)\n    \n    def _generate_fallback_analysis(self, content_data: Dict[str, Any], weights: CriteriaWeight) -> Dict[str, Any]:\n        \"\"\"Генерация fallback анализа\"\"\"\n        # Упрощенный анализ на основе ключевых слов\n        text = content_data.get(\"text\", \"\").lower()\n        \n        # Базовые оценки с корректировкой по ключевым словам\n        scores = {\n            \"budget_compliance\": 75 + (5 if any(kw in text for kw in [\"бюджет\", \"стоимость\", \"цена\"]) else 0),\n            \"timeline_compliance\": 78 + (5 if any(kw in text for kw in [\"срок\", \"время\", \"этап\"]) else 0),\n            \"technical_compliance\": 80 + (5 if any(kw in text for kw in [\"технология\", \"архитектура\", \"api\"]) else 0),\n            \"team_expertise\": 72 + (5 if any(kw in text for kw in [\"команда\", \"опыт\", \"эксперт\"]) else 0),\n            \"functional_coverage\": 76 + (5 if any(kw in text for kw in [\"функция\", \"модуль\", \"система\"]) else 0),\n            \"quality_assurance\": 70 + (5 if any(kw in text for kw in [\"качество\", \"тест\", \"контроль\"]) else 0),\n            \"development_methodology\": 68 + (5 if any(kw in text for kw in [\"agile\", \"scrum\", \"методология\"]) else 0),\n            \"scalability\": 74 + (5 if any(kw in text for kw in [\"масштаб\", \"рост\", \"развитие\"]) else 0),\n            \"communication\": 71 + (5 if any(kw in text for kw in [\"связь\", \"отчет\", \"коммуникация\"]) else 0),\n            \"added_value\": 69 + (5 if any(kw in text for kw in [\"преимущество\", \"бонус\", \"дополнительно\"]) else 0)\n        }\n        \n        # Рассчитываем взвешенную оценку\n        weighted_score = sum(\n            scores[criterion.replace(\"_compliance\", \"_compliance\")] * getattr(weights, criterion)\n            for criterion in scores.keys()\n        )\n        \n        overall_score = int(sum(scores.values()) / len(scores))\n        \n        return {\n            \"company_name\": \"Анализируемая компания\",\n            \"overall_score\": overall_score,\n            \"weighted_score\": round(weighted_score, 2),\n            \"executive_summary\": f\"Проведен экспертный анализ по 10 критериям. Взвешенная оценка: {weighted_score:.1f}/100\",\n            \"business_analysis\": self._build_detailed_criteria(scores, weights),\n            \"recommendations\": [\n                \"Рекомендуется к рассмотрению\" if overall_score >= 75 else \"Требует доработки\",\n                \"Проверить техническую реализуемость\",\n                \"Уточнить временные рамки проекта\"\n            ],\n            \"risk_level\": \"Низкий\" if overall_score >= 80 else \"Умеренный\" if overall_score >= 65 else \"Высокий\"\n        }\n    \n    def _build_detailed_criteria(self, scores: Dict[str, int], weights: CriteriaWeight) -> Dict[str, Any]:\n        \"\"\"Построение детального анализа критериев\"\"\"\n        criteria_mapping = {\n            \"budget_compliance\": (weights.budget_compliance, \"Соответствие бюджетным требованиям\"),\n            \"timeline_compliance\": (weights.timeline_compliance, \"Соответствие временным рамкам\"),\n            \"technical_compliance\": (weights.technical_compliance, \"Техническое соответствие\"),\n            \"team_expertise\": (weights.team_expertise, \"Экспертиза команды\"),\n            \"functional_coverage\": (weights.functional_coverage, \"Функциональное покрытие\"),\n            \"quality_assurance\": (weights.quality_assurance, \"Обеспечение качества\"),\n            \"development_methodology\": (weights.development_methodology, \"Методология разработки\"),\n            \"scalability\": (weights.scalability, \"Масштабируемость\"),\n            \"communication\": (weights.communication, \"Коммуникация\"),\n            \"added_value\": (weights.added_value, \"Добавленная стоимость\")\n        }\n        \n        result = {}\n        for criterion, score in scores.items():\n            weight, description = criteria_mapping[criterion]\n            compliance_level = \"high\" if score >= 80 else \"medium\" if score >= 65 else \"low\"\n            \n            result[criterion] = {\n                \"score\": min(score, 100),\n                \"weight\": weight,\n                \"details\": f\"{description}. Оценка: {score}/100\",\n                \"recommendations\": [\n                    \"Соответствует требованиям\" if score >= 75 else \"Требует улучшения\",\n                    \"Рекомендуется детальная проработка\"\n                ],\n                \"compliance_level\": compliance_level,\n                \"risk_factors\": [] if score >= 75 else [\"Потенциальные проблемы по критерию\"]\n            }\n        \n        return result\n    \n    async def _generate_analysis_charts(self, analysis_data: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Генерация диаграмм для анализа\"\"\"\n        try:\n            charts = {}\n            \n            business_analysis = analysis_data.get(\"business_analysis\", {})\n            if business_analysis:\n                # Радарная диаграмма критериев\n                criteria_names = []\n                criteria_scores = []\n                \n                for criterion, data in business_analysis.items():\n                    criteria_names.append(criterion.replace(\"_\", \" \").title())\n                    criteria_scores.append(data.get(\"score\", 0))\n                \n                charts[\"criteria_radar\"] = {\n                    \"type\": \"radar\",\n                    \"data\": {\n                        \"categories\": criteria_names,\n                        \"scores\": criteria_scores\n                    }\n                }\n                \n                # Столбчатая диаграмма\n                charts[\"criteria_bar\"] = {\n                    \"type\": \"bar\",\n                    \"data\": {\n                        \"categories\": criteria_names,\n                        \"scores\": criteria_scores,\n                        \"colors\": [\n                            \"#22c55e\" if score >= 80 else \"#3b82f6\" if score >= 65 else \"#f59e0b\" if score >= 50 else \"#ef4444\"\n                            for score in criteria_scores\n                        ]\n                    }\n                }\n            \n            return charts\n            \n        except Exception as e:\n            logger.error(f\"Chart generation error: {e}\")\n            return {}\n    \n    async def _save_uploaded_file(self, file: UploadFile, content: bytes) -> Path:\n        \"\"\"Сохранение загруженного файла\"\"\"\n        uploads_dir = Path(\"data/uploads\")\n        uploads_dir.mkdir(parents=True, exist_ok=True)\n        \n        # Генерируем уникальное имя файла\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        safe_filename = self._sanitize_filename(file.filename)\n        unique_filename = f\"{timestamp}_{safe_filename}\"\n        \n        file_path = uploads_dir / unique_filename\n        \n        with open(file_path, 'wb') as f:\n            f.write(content)\n        \n        return file_path\n    \n    def _generate_unique_filename(self, original: str) -> str:\n        \"\"\"Генерация уникального имени файла\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        safe_name = self._sanitize_filename(original)\n        return f\"{timestamp}_{safe_name}\"\n    \n    def _sanitize_filename(self, filename: str) -> str:\n        \"\"\"Очистка имени файла от опасных символов\"\"\"\n        import re\n        # Оставляем только буквы, цифры, точки и дефисы\n        safe_name = re.sub(r'[^\\w\\.-]', '_', filename)\n        return safe_name[:100]  # Ограничиваем длину\n    \n    def _get_file_type(self, filename: str) -> str:\n        \"\"\"Определение типа файла по расширению\"\"\"\n        extension = Path(filename).suffix.lower()\n        type_mapping = {\n            '.pdf': 'pdf',\n            '.docx': 'docx', \n            '.doc': 'doc',\n            '.txt': 'txt',\n            '.xlsx': 'xlsx',\n            '.xls': 'xls'\n        }\n        return type_mapping.get(extension, 'unknown')"