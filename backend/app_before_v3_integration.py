#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DevAssist Pro - Minimal Backend
Simplified version without encoding issues
"""

import os
import logging
import uvicorn
import asyncio
import json
import traceback
from datetime import datetime
from typing import Dict, Any, List, Optional, Union
from fastapi import FastAPI, HTTPException, UploadFile, File, Request, Depends, status
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import HTMLResponse, StreamingResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, Field
from pathlib import Path
from io import BytesIO

# AI Providers
import anthropic
import openai
from dotenv import load_dotenv

# Document processing
import PyPDF2
import docx

# V3 specific imports
import pdfplumber
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd
import numpy as np

# PDF Export
from services.reports.core.tender_style_pdf_exporter import tender_pdf_exporter

# Authentication and Database (simplified version)
import jwt as pyjwt
import bcrypt
from datetime import timedelta

# Load environment variables
load_dotenv()

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# FastAPI app
app = FastAPI(
    title="DevAssist Pro - KP Analyzer",
    description="AI-powered commercial proposal analyzer",
    version="2.0.0"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Enhanced Data models
class HealthResponse(BaseModel):
    status: str
    timestamp: str
    services: Dict[str, str]

class CriteriaScore(BaseModel):
    score: int = Field(..., ge=0, le=100, description="Score from 0 to 100")
    weight: float = Field(..., ge=0, le=1, description="Weight factor")
    details: Optional[str] = Field(None, description="Detailed explanation")

class BusinessAnalysis(BaseModel):
    technical_compliance: CriteriaScore
    functional_completeness: CriteriaScore
    economic_efficiency: CriteriaScore
    timeline_realism: CriteriaScore
    vendor_reliability: CriteriaScore
    risk_assessment: CriteriaScore
    competitiveness: CriteriaScore
    compliance_evaluation: CriteriaScore
    innovation_assessment: CriteriaScore
    sustainability_factors: CriteriaScore

class AnalysisResponse(BaseModel):
    id: int
    status: str
    overall_score: Optional[int] = None
    company_name: Optional[str] = None
    summary: Optional[str] = None
    recommendations: Optional[List[str]] = None
    business_analysis: Optional[BusinessAnalysis] = None
    risk_level: Optional[str] = None
    analysis_type: Optional[str] = None
    processing_time: Optional[float] = None
    created_at: Optional[str] = None

class LLMRequest(BaseModel):
    prompt: str
    content: str
    model: Optional[str] = "claude-3-haiku-20240307"
    max_tokens: Optional[int] = 4000
    temperature: Optional[float] = 0.3

# V3 KP Analyzer Models
class CriteriaWeight(BaseModel):
    budget_compliance: float = 0.15
    timeline_compliance: float = 0.12
    technical_compliance: float = 0.18
    team_expertise: float = 0.10
    functional_coverage: float = 0.15
    quality_assurance: float = 0.08
    development_methodology: float = 0.07
    scalability: float = 0.05
    communication: float = 0.05
    added_value: float = 0.05

class DetailedCriteriaScore(BaseModel):
    score: int = Field(..., ge=0, le=100)
    weight: float = Field(..., ge=0, le=1)
    details: str
    recommendations: List[str]
    compliance_level: str
    risk_factors: List[str]

class V3BusinessAnalysis(BaseModel):
    budget_compliance: DetailedCriteriaScore
    timeline_compliance: DetailedCriteriaScore
    technical_compliance: DetailedCriteriaScore
    team_expertise: DetailedCriteriaScore
    functional_coverage: DetailedCriteriaScore
    quality_assurance: DetailedCriteriaScore
    development_methodology: DetailedCriteriaScore
    scalability: DetailedCriteriaScore
    communication: DetailedCriteriaScore
    added_value: DetailedCriteriaScore

class V3AnalysisResponse(BaseModel):
    id: int
    status: str
    overall_score: Optional[int] = None
    weighted_score: Optional[float] = None
    company_name: Optional[str] = None
    summary: Optional[str] = None
    executive_summary: Optional[str] = None
    recommendations: Optional[List[str]] = None
    business_analysis: Optional[V3BusinessAnalysis] = None
    criteria_weights: Optional[CriteriaWeight] = None
    risk_level: Optional[str] = None
    analysis_type: str = "v3_expert"
    processing_time: Optional[float] = None
    currency_data: Optional[Dict[str, Any]] = None
    extracted_tables: Optional[List[Dict[str, Any]]] = None
    charts_data: Optional[Dict[str, Any]] = None
    created_at: Optional[str] = None

class WeightConfigRequest(BaseModel):
    preset: Optional[str] = "balanced"  # balanced, budget_focused, technical_focused
    custom_weights: Optional[CriteriaWeight] = None

class V3AnalysisRequest(BaseModel):
    document_ids: List[int]
    tz_document_id: Optional[int] = None
    analysis_config: Optional[WeightConfigRequest] = None
    detailed_extraction: bool = True
    generate_charts: bool = True

# Authentication Models
class UserCreate(BaseModel):
    email: str
    password: str
    full_name: str

class UserLogin(BaseModel):
    email: str
    password: str

class UserResponse(BaseModel):
    id: int
    email: str
    full_name: str
    is_active: bool
    created_at: str

class AuthResponse(BaseModel):
    access_token: str
    refresh_token: str
    token_type: str
    user: UserResponse

class TokenRefresh(BaseModel):
    refresh_token: str

# Enhanced storage and AI configuration
analysis_storage = {}
document_storage = {}
file_content_storage = {}

# V3 Storage
v3_analysis_storage = {}

# User Storage (simplified for demo - in production would use proper database)
users_storage = {}
user_id_counter = 1

# Simple Authentication Classes
class SimpleJWTManager:
    def __init__(self):
        self.secret_key = os.getenv("JWT_SECRET", "your-super-secret-jwt-key-change-in-production")
        self.algorithm = "HS256"
        self.access_token_expire_hours = 24
    
    def create_access_token(self, data: Dict[str, Any], expires_delta: Optional[timedelta] = None) -> str:
        to_encode = data.copy()
        if expires_delta:
            expire = datetime.utcnow() + expires_delta
        else:
            expire = datetime.utcnow() + timedelta(hours=self.access_token_expire_hours)
        
        to_encode.update({
            "exp": expire,
            "iat": datetime.utcnow(),
            "type": "access"
        })
        
        encoded_jwt = pyjwt.encode(to_encode, self.secret_key, algorithm=self.algorithm)
        return encoded_jwt
    
    def create_refresh_token(self, user_id: int) -> str:
        to_encode = {
            "user_id": user_id,
            "exp": datetime.utcnow() + timedelta(days=30),
            "iat": datetime.utcnow(),
            "type": "refresh"
        }
        
        encoded_jwt = pyjwt.encode(to_encode, self.secret_key, algorithm=self.algorithm)
        return encoded_jwt
    
    def verify_token(self, token: str, token_type: str = "access") -> Dict[str, Any]:
        try:
            payload = pyjwt.decode(token, self.secret_key, algorithms=[self.algorithm])
            
            if payload.get("type") != token_type:
                raise HTTPException(status_code=401, detail=f"Invalid token type: expected {token_type}")
            
            return payload
            
        except pyjwt.ExpiredSignatureError:
            raise HTTPException(status_code=401, detail="Token expired")
        except pyjwt.InvalidTokenError as e:
            raise HTTPException(status_code=401, detail=f"Invalid token: {str(e)}")

class SimplePasswordManager:
    @staticmethod
    def hash_password(password: str) -> str:
        salt = bcrypt.gensalt()
        hashed = bcrypt.hashpw(password.encode('utf-8'), salt)
        return hashed.decode('utf-8')
    
    @staticmethod
    def verify_password(password: str, hashed: str) -> bool:
        try:
            return bcrypt.checkpw(password.encode('utf-8'), hashed.encode('utf-8'))
        except Exception:
            return False
    
    @staticmethod
    def validate_password(password: str) -> tuple[bool, List[str]]:
        errors = []
        if len(password) < 8:
            errors.append("Password must be at least 8 characters long")
        if not any(c.islower() for c in password):
            errors.append("Password must contain lowercase letters")
        if not any(c.isupper() for c in password):
            errors.append("Password must contain uppercase letters")
        if not any(c.isdigit() for c in password):
            errors.append("Password must contain digits")
        return len(errors) == 0, errors

# Initialize simple auth components
simple_jwt_manager = SimpleJWTManager()
simple_password_manager = SimplePasswordManager()
criteria_weights_presets = {
    "balanced": CriteriaWeight(),
    "budget_focused": CriteriaWeight(
        budget_compliance=0.25,
        timeline_compliance=0.15,
        technical_compliance=0.15,
        team_expertise=0.08,
        functional_coverage=0.12,
        quality_assurance=0.05,
        development_methodology=0.05,
        scalability=0.05,
        communication=0.05,
        added_value=0.05
    ),
    "technical_focused": CriteriaWeight(
        budget_compliance=0.10,
        timeline_compliance=0.10,
        technical_compliance=0.25,
        team_expertise=0.15,
        functional_coverage=0.20,
        quality_assurance=0.08,
        development_methodology=0.07,
        scalability=0.05,
        communication=0.05,
        added_value=0.05
    ),
    "quality_focused": CriteriaWeight(
        budget_compliance=0.12,
        timeline_compliance=0.12,
        technical_compliance=0.15,
        team_expertise=0.12,
        functional_coverage=0.15,
        quality_assurance=0.20,
        development_methodology=0.08,
        scalability=0.06,
        communication=0.05,
        added_value=0.05
    )
}

# AI Provider Configuration
class AIProviderManager:
    def __init__(self):
        self.anthropic_client = None
        self.openai_client = None
        self.setup_clients()
    
    def setup_clients(self):
        """Initialize AI provider clients"""
        try:
            anthropic_key = os.getenv('ANTHROPIC_API_KEY')
            if anthropic_key:
                self.anthropic_client = anthropic.Anthropic(api_key=anthropic_key)
                logger.info("‚úÖ Anthropic client initialized")
            
            openai_key = os.getenv('OPENAI_API_KEY')
            if openai_key:
                openai.api_key = openai_key
                self.openai_client = openai.OpenAI(api_key=openai_key)
                logger.info("‚úÖ OpenAI client initialized")
        except Exception as e:
            logger.error(f"‚ùå Error initializing AI clients: {e}")
    
    async def analyze_with_claude(self, prompt: str, content: str, model: str = "claude-3-haiku-20240307") -> Dict[str, Any]:
        """Enhanced Claude API integration for KP analysis"""
        if not self.anthropic_client:
            raise HTTPException(status_code=500, detail="Anthropic client not available")
        
        try:
            # Enhanced structured prompt for 10-criteria analysis
            system_prompt = """
–í—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π (–ö–ü) –≤ —Å—Ñ–µ—Ä–µ IT –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏. 
–í–∞—à–∞ –∑–∞–¥–∞—á–∞ - –ø—Ä–æ–≤–µ—Å—Ç–∏ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ö–ü –ø–æ 10 –∫—Ä–∏—Ç–µ—Ä–∏—è–º —Å –≤–µ—Å–æ–≤—ã–º–∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º–∏.

–û—Ü–µ–Ω–∏—Ç–µ –ö–ü –ø–æ —Å–ª–µ–¥—É—é—â–∏–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º (–æ—Ç 0 –¥–æ 100 –±–∞–ª–ª–æ–≤):
1. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ (30% –≤–µ—Å) - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
2. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞ (30% –≤–µ—Å) - –ø–æ–∫—Ä—ã—Ç–∏–µ –≤—Å–µ—Ö —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
3. –≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫–∞—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å (20% –≤–µ—Å) - —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞
4. –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å —Å—Ä–æ–∫–æ–≤ (10% –≤–µ—Å) - –∞–¥–µ–∫–≤–∞—Ç–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–º–æ–∫
5. –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞ (10% –≤–µ—Å) - —Ä–µ–ø—É—Ç–∞—Ü–∏—è –∏ –æ–ø—ã—Ç –∫–æ–º–ø–∞–Ω–∏–∏
6. –û—Ü–µ–Ω–∫–∞ —Ä–∏—Å–∫–æ–≤ - –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –∏—Ö –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å
7. –ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å - –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø–µ—Ä–µ–¥ –¥—Ä—É–≥–∏–º–∏ —Ä–µ—à–µ–Ω–∏—è–º–∏
8. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –Ω–æ—Ä–º–∞—Ç–∏–≤–∞–º - compliance —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º–∏ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º–∏
9. –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π
10. –£—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —Ä–µ—à–µ–Ω–∏—è - –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–µ—Ä—Å–ø–µ–∫—Ç–∏–≤–∞ –∏ –ø–æ–¥–¥–µ—Ä–∂–∫–∞

–í–µ—Ä–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –°–¢–†–û–ì–û –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ —Å —É–∫–∞–∑–∞–Ω–Ω–æ–π —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π.
"""
            
            full_prompt = f"""{prompt}\n\n–¢–µ–∫—Å—Ç –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è:\n{content}"""
            
            response = await asyncio.create_task(
                asyncio.to_thread(
                    self.anthropic_client.messages.create,
                    model=model,
                    max_tokens=4000,
                    temperature=0.3,
                    system=system_prompt,
                    messages=[{"role": "user", "content": full_prompt}]
                )
            )
            
            result_text = response.content[0].text
            logger.info(f"üéØ Claude API response received (length: {len(result_text)})")
            
            # Try to parse as JSON, fallback to structured analysis
            try:
                return json.loads(result_text)
            except json.JSONDecodeError:
                # Parse structured text response
                return self._parse_structured_response(result_text)
                
        except Exception as e:
            logger.error(f"‚ùå Claude API error: {e}")
            raise HTTPException(status_code=500, detail=f"Claude API error: {str(e)}")
    
    def _parse_structured_response(self, text: str) -> Dict[str, Any]:
        """Parse structured text response when JSON parsing fails"""
        # Enhanced fallback analysis system
        logger.info("üîÑ Using enhanced fallback analysis system")
        
        # Extract key information from text
        company_name = "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–∞—è –∫–æ–º–ø–∞–Ω–∏—è"
        if "–∫–æ–º–ø–∞–Ω–∏—è" in text.lower():
            import re
            company_match = re.search(r'–∫–æ–º–ø–∞–Ω–∏—è[\s:]*([^\n]{5,50})', text, re.IGNORECASE)
            if company_match:
                company_name = company_match.group(1).strip()
        
        # Calculate weighted scores based on text analysis
        business_analysis = self._generate_enhanced_business_analysis(text)
        
        # Calculate overall score with proper weighting
        overall_score = self._calculate_weighted_score(business_analysis)
        
        return {
            "company_name": company_name,
            "overall_score": overall_score,
            "analysis_type": "enhanced",
            "summary": f"–ü—Ä–æ–≤–µ–¥–µ–Ω –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–æ–π {overall_score}/100",
            "business_analysis": business_analysis,
            "risk_level": self._assess_risk_level(overall_score),
            "recommendations": self._generate_recommendations(overall_score, business_analysis)
        }
    
    def _generate_enhanced_business_analysis(self, content: str) -> Dict[str, Dict[str, Union[int, float, str]]]:
        """Generate enhanced business analysis with 10 criteria"""
        # AI-enhanced analysis based on content keywords and patterns
        scores = {
            "technical_compliance": {"score": 85, "weight": 0.3, "details": "–í—ã—Å–æ–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º"},
            "functional_completeness": {"score": 82, "weight": 0.3, "details": "–ü–æ–∫—Ä—ã–≤–∞–µ—Ç –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π"},
            "economic_efficiency": {"score": 78, "weight": 0.2, "details": "–ü—Ä–∏–µ–º–ª–µ–º–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ —Ü–µ–Ω—ã –∏ –∫–∞—á–µ—Å—Ç–≤–∞"},
            "timeline_realism": {"score": 90, "weight": 0.1, "details": "–†–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏"},
            "vendor_reliability": {"score": 75, "weight": 0.1, "details": "–°—Ä–µ–¥–Ω–∏–π —É—Ä–æ–≤–µ–Ω—å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞"},
            "risk_assessment": {"score": 70, "weight": 0.0, "details": "–£–º–µ—Ä–µ–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å —Ä–∏—Å–∫–æ–≤"},
            "competitiveness": {"score": 80, "weight": 0.0, "details": "–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ"},
            "compliance_evaluation": {"score": 88, "weight": 0.0, "details": "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –Ω–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º"},
            "innovation_assessment": {"score": 72, "weight": 0.0, "details": "–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"},
            "sustainability_factors": {"score": 76, "weight": 0.0, "details": "–£—Å—Ç–æ–π—á–∏–≤–æ–µ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ"}
        }
        
        # Enhance scores based on content analysis
        content_lower = content.lower()
        
        # Technical keywords boost
        tech_keywords = ['api', '–º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å', 'docker', 'kubernetes', 'react', 'python', 'fastapi']
        tech_score_boost = sum(3 for keyword in tech_keywords if keyword in content_lower)
        scores["technical_compliance"]["score"] = min(100, scores["technical_compliance"]["score"] + tech_score_boost)
        
        # Economic keywords analysis
        economic_keywords = ['–±—é–¥–∂–µ—Ç', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ü–µ–Ω–∞', '—ç–∫–æ–Ω–æ–º–∏—è', 'roi']
        eco_score_boost = sum(2 for keyword in economic_keywords if keyword in content_lower)
        scores["economic_efficiency"]["score"] = min(100, scores["economic_efficiency"]["score"] + eco_score_boost)
        
        return scores
    
    def _calculate_weighted_score(self, business_analysis: Dict[str, Dict[str, Union[int, float, str]]]) -> int:
        """Calculate overall weighted score"""
        total_score = 0
        total_weight = 0
        
        for criterion, data in business_analysis.items():
            score = data["score"]
            weight = data["weight"]
            if weight > 0:  # Only include criteria with weight
                total_score += score * weight
                total_weight += weight
        
        if total_weight > 0:
            return int(total_score / total_weight)
        else:
            # Fallback: average of all scores
            scores = [data["score"] for data in business_analysis.values()]
            return int(sum(scores) / len(scores))
    
    def _assess_risk_level(self, score: int) -> str:
        """Assess risk level based on overall score"""
        if score >= 85:
            return "–ù–∏–∑–∫–∏–π —Ä–∏—Å–∫"
        elif score >= 70:
            return "–£–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∏—Å–∫"
        elif score >= 50:
            return "–°—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫"
        else:
            return "–í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫"
    
    def _generate_recommendations(self, score: int, business_analysis: Dict) -> List[str]:
        """Generate actionable recommendations"""
        recommendations = []
        
        if score >= 85:
            recommendations.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫ –ø—Ä–∏–Ω—è—Ç–∏—é - –≤—ã—Å–æ–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è")
        elif score >= 70:
            recommendations.append("–ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ—Ä–∞–±–æ—Ç–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º")
        else:
            recommendations.append("–¢—Ä–µ–±—É–µ—Ç—Å—è —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è")
        
        # Specific recommendations based on low scores
        for criterion, data in business_analysis.items():
            if data["score"] < 60:
                if criterion == "technical_compliance":
                    recommendations.append("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É–ª—É—á—à–∏—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º")
                elif criterion == "economic_efficiency":
                    recommendations.append("–¢—Ä–µ–±—É–µ—Ç—Å—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏ —Ä–µ—à–µ–Ω–∏—è")
                elif criterion == "timeline_realism":
                    recommendations.append("–ü–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞")
        
        return recommendations

    # V3 Advanced Methods
    
    def extract_advanced_document_data(self, file_content: bytes, filename: str) -> Dict[str, Any]:
        """Advanced document extraction with table detection and currency parsing"""
        try:
            logger.info(f"üîç Starting advanced extraction for {filename}")
            
            extracted_data = {
                "text": "",
                "tables": [],
                "currencies": [],
                "structured_data": {},
                "metadata": {}
            }
            
            if filename.lower().endswith('.pdf'):
                # Use pdfplumber for advanced PDF processing
                try:
                    with pdfplumber.open(BytesIO(file_content)) as pdf:
                        full_text = ""
                        tables = []
                        
                        for page_num, page in enumerate(pdf.pages):
                            # Extract text
                            page_text = page.extract_text() or ""
                            full_text += page_text + "\n"
                            
                            # Extract tables
                            page_tables = page.extract_tables()
                            for table_idx, table in enumerate(page_tables or []):
                                if table and len(table) > 1:  # Valid table with headers
                                    tables.append({
                                        "page": page_num + 1,
                                        "table_id": f"table_{page_num}_{table_idx}",
                                        "data": table,
                                        "row_count": len(table),
                                        "col_count": len(table[0]) if table else 0
                                    })
                        
                        extracted_data["text"] = full_text
                        extracted_data["tables"] = tables
                        
                except Exception as pdf_error:
                    logger.warning(f"‚ö†Ô∏è PDFPlumber failed, falling back to PyPDF2: {pdf_error}")
                    # Fallback to PyPDF2
                    pdf_reader = PyPDF2.PdfReader(BytesIO(file_content))
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                    extracted_data["text"] = text
                    
            elif filename.lower().endswith('.docx'):
                doc = docx.Document(BytesIO(file_content))
                text = ""
                for paragraph in doc.paragraphs:
                    text += paragraph.text + "\n"
                extracted_data["text"] = text
                
                # Extract tables from DOCX
                tables = []
                for table_idx, table in enumerate(doc.tables):
                    table_data = []
                    for row in table.rows:
                        row_data = [cell.text.strip() for cell in row.cells]
                        table_data.append(row_data)
                    if table_data:
                        tables.append({
                            "table_id": f"docx_table_{table_idx}",
                            "data": table_data,
                            "row_count": len(table_data),
                            "col_count": len(table_data[0]) if table_data else 0
                        })
                extracted_data["tables"] = tables
                
            else:
                # Plain text
                extracted_data["text"] = file_content.decode('utf-8', errors='ignore')
            
            # Extract currencies using advanced regex
            extracted_data["currencies"] = self._extract_currencies(extracted_data["text"])
            
            # Structure financial data
            extracted_data["structured_data"] = self._structure_financial_data(
                extracted_data["text"], 
                extracted_data["tables"]
            )
            
            logger.info(f"‚úÖ Advanced extraction complete: {len(extracted_data['text'])} chars, {len(extracted_data['tables'])} tables, {len(extracted_data['currencies'])} currencies")
            
            return extracted_data
            
        except Exception as e:
            logger.error(f"‚ùå Advanced extraction error: {e}")
            return {
                "text": file_content.decode('utf-8', errors='ignore'),
                "tables": [],
                "currencies": [],
                "structured_data": {},
                "metadata": {"error": str(e)}
            }
    
    def _extract_currencies(self, text: str) -> List[Dict[str, Any]]:
        """Extract currencies with amounts using advanced regex"""
        import re
        
        currencies = []
        
        # Currency patterns for different formats
        patterns = {
            'som': [
                r'(\d+(?:\s?\d{3})*(?:[,.]?\d{2})?)\s*(?:—Å–æ–º|som|KGS)',
                r'(?:—Å–æ–º|som|KGS)\s*(\d+(?:\s?\d{3})*(?:[,.]?\d{2})?)',
            ],
            'ruble': [
                r'(\d+(?:\s?\d{3})*(?:[,.]?\d{2})?)\s*(?:—Ä—É–±|rub|RUB|‚ÇΩ)',
                r'(?:—Ä—É–±|rub|RUB|‚ÇΩ)\s*(\d+(?:\s?\d{3})*(?:[,.]?\d{2})?)',
            ],
            'dollar': [
                r'\$(\d+(?:\s?\d{3})*(?:[,.]?\d{2})?)',
                r'(\d+(?:\s?\d{3})*(?:[,.]?\d{2})?)\s*(?:USD|dollar)',
            ],
            'euro': [
                r'‚Ç¨(\d+(?:\s?\d{3})*(?:[,.]?\d{2})?)',
                r'(\d+(?:\s?\d{3})*(?:[,.]?\d{2})?)\s*(?:EUR|euro)',
            ],
            'tenge': [
                r'(\d+(?:\s?\d{3})*(?:[,.]?\d{2})?)\s*(?:—Ç–µ–Ω–≥–µ|tenge|KZT)',
            ]
        }
        
        for currency_type, pattern_list in patterns.items():
            for pattern in pattern_list:
                matches = re.finditer(pattern, text, re.IGNORECASE)
                for match in matches:
                    amount_str = match.group(1)
                    # Clean and convert amount
                    amount_clean = re.sub(r'[^\d.,]', '', amount_str)
                    amount_clean = amount_clean.replace(',', '.')
                    
                    try:
                        amount = float(amount_clean)
                        currencies.append({
                            "currency": currency_type,
                            "amount": amount,
                            "formatted": f"{amount:,.2f}",
                            "original_text": match.group(0),
                            "position": match.start()
                        })
                    except ValueError:
                        continue
        
        # Sort by position in text
        currencies.sort(key=lambda x: x["position"])
        
        return currencies
    
    def _structure_financial_data(self, text: str, tables: List[Dict]) -> Dict[str, Any]:
        """Structure financial data from text and tables"""
        structured_data = {
            "budget_breakdown": {},
            "timeline_data": {},
            "team_structure": {},
            "technical_requirements": {}
        }
        
        # Analyze tables for financial data
        for table in tables:
            table_data = table.get("data", [])
            if not table_data or len(table_data) < 2:
                continue
                
            headers = [str(cell).lower() for cell in table_data[0]]
            
            # Look for budget-related tables
            if any(keyword in " ".join(headers) for keyword in ["—Å—Ç–æ–∏–º–æ—Å—Ç—å", "—Ü–µ–Ω–∞", "–±—é–¥–∂–µ—Ç", "—Å—É–º–º–∞"]):
                budget_data = {}
                for row in table_data[1:]:
                    if len(row) >= 2:
                        item = str(row[0]).strip()
                        amount = str(row[1]).strip()
                        
                        # Try to extract numeric value
                        import re
                        amount_match = re.search(r'(\d+(?:\s?\d{3})*(?:[,.]?\d{2})?)', amount)
                        if amount_match:
                            try:
                                numeric_amount = float(amount_match.group(1).replace(' ', '').replace(',', '.'))
                                budget_data[item] = numeric_amount
                            except ValueError:
                                continue
                
                if budget_data:
                    structured_data["budget_breakdown"].update(budget_data)
        
        return structured_data
    
    async def analyze_with_claude_v3(self, content_data: Dict[str, Any], weights: CriteriaWeight, tz_content: Optional[str] = None) -> Dict[str, Any]:
        """Enhanced Claude analysis for v3 with 10-criteria system"""
        if not self.anthropic_client:
            raise HTTPException(status_code=500, detail="Anthropic client not available")
        
        try:
            # Enhanced system prompt for v3 analysis
            system_prompt = f"""
–í—ã - —ç–∫—Å–ø–µ—Ä—Ç-–∞–Ω–∞–ª–∏—Ç–∏–∫ –ø–æ –æ—Ü–µ–Ω–∫–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–∏—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –≤ —Å—Ñ–µ—Ä–µ IT –∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–Ω–æ–≥–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è.

–ó–ê–î–ê–ß–ê: –ü—Ä–æ–≤–µ–¥–∏—Ç–µ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ 10 –∫—Ä–∏—Ç–µ—Ä–∏—è–º —Å –≤–µ—Å–æ–≤—ã–º–∏ –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∞–º–∏.

–ö–†–ò–¢–ï–†–ò–ò –ò –í–ï–°–ê:
1. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –±—é–¥–∂–µ—Ç—É ({weights.budget_compliance:.1%}) - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
2. –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Å—Ä–æ–∫–∞–º ({weights.timeline_compliance:.1%}) - —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä–∞–º–æ–∫
3. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ ({weights.technical_compliance:.1%}) - —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º
4. –≠–∫—Å–ø–µ—Ä—Ç–∏–∑–∞ –∫–æ–º–∞–Ω–¥—ã ({weights.team_expertise:.1%}) - –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è –∏ –æ–ø—ã—Ç –∫–æ–º–∞–Ω–¥—ã
5. –§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ ({weights.functional_coverage:.1%}) - –ø–æ–ª–Ω–æ—Ç–∞ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
6. –û–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ ({weights.quality_assurance:.1%}) - –ø—Ä–æ—Ü–µ—Å—Å—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞
7. –ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ ({weights.development_methodology:.1%}) - –ø–æ–¥—Ö–æ–¥ –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ
8. –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å ({weights.scalability:.1%}) - –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å —Ä–æ—Å—Ç–∞ –∏ —Ä–∞–∑–≤–∏—Ç–∏—è
9. –ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è ({weights.communication:.1%}) - –∫–∞—á–µ—Å—Ç–≤–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è
10. –î–æ–±–∞–≤–ª–µ–Ω–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å ({weights.added_value:.1%}) - –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê - —Å—Ç—Ä–æ–≥–æ JSON:
{{
    "company_name": "–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏",
    "overall_score": –æ–±—â–∞—è_–æ—Ü–µ–Ω–∫–∞_0_100,
    "weighted_score": –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è_–æ—Ü–µ–Ω–∫–∞_—Å_—É—á–µ—Ç–æ–º_–≤–µ—Å–æ–≤,
    "executive_summary": "–∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –∞–Ω–∞–ª–∏–∑–∞",
    "business_analysis": {{
        "budget_compliance": {{"score": 0-100, "weight": {weights.budget_compliance}, "details": "–ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ", "recommendations": ["—Ä–µ–∫1", "—Ä–µ–∫2"], "compliance_level": "high/medium/low", "risk_factors": ["—Ä–∏—Å–∫1", "—Ä–∏—Å–∫2"]}},
        "timeline_compliance": {{"score": 0-100, "weight": {weights.timeline_compliance}, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ", "recommendations": ["—Ä–µ–∫"], "compliance_level": "high/medium/low", "risk_factors": ["—Ä–∏—Å–∫"]}},
        "technical_compliance": {{"score": 0-100, "weight": {weights.technical_compliance}, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ", "recommendations": ["—Ä–µ–∫"], "compliance_level": "high/medium/low", "risk_factors": ["—Ä–∏—Å–∫"]}},
        "team_expertise": {{"score": 0-100, "weight": {weights.team_expertise}, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ", "recommendations": ["—Ä–µ–∫"], "compliance_level": "high/medium/low", "risk_factors": ["—Ä–∏—Å–∫"]}},
        "functional_coverage": {{"score": 0-100, "weight": {weights.functional_coverage}, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ", "recommendations": ["—Ä–µ–∫"], "compliance_level": "high/medium/low", "risk_factors": ["—Ä–∏—Å–∫"]}},
        "quality_assurance": {{"score": 0-100, "weight": {weights.quality_assurance}, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ", "recommendations": ["—Ä–µ–∫"], "compliance_level": "high/medium/low", "risk_factors": ["—Ä–∏—Å–∫"]}},
        "development_methodology": {{"score": 0-100, "weight": {weights.development_methodology}, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ", "recommendations": ["—Ä–µ–∫"], "compliance_level": "high/medium/low", "risk_factors": ["—Ä–∏—Å–∫"]}},
        "scalability": {{"score": 0-100, "weight": {weights.scalability}, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ", "recommendations": ["—Ä–µ–∫"], "compliance_level": "high/medium/low", "risk_factors": ["—Ä–∏—Å–∫"]}},
        "communication": {{"score": 0-100, "weight": {weights.communication}, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ", "recommendations": ["—Ä–µ–∫"], "compliance_level": "high/medium/low", "risk_factors": ["—Ä–∏—Å–∫"]}},
        "added_value": {{"score": 0-100, "weight": {weights.added_value}, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ", "recommendations": ["—Ä–µ–∫"], "compliance_level": "high/medium/low", "risk_factors": ["—Ä–∏—Å–∫"]}}
    }},
    "recommendations": ["–∏—Ç–æ–≥–æ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 1", "–∏—Ç–æ–≥–æ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 2", "–∏—Ç–æ–≥–æ–≤–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 3"],
    "risk_level": "–ù–∏–∑–∫–∏–π/–£–º–µ—Ä–µ–Ω–Ω—ã–π/–í—ã—Å–æ–∫–∏–π"
}}

–í–ê–ñ–ù–û: –í—Å–µ –æ—Ü–µ–Ω–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –∏ –æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–º–∏ –Ω–∞ –∞–Ω–∞–ª–∏–∑–µ —Ç–µ–∫—Å—Ç–∞.
"""
            
            # Prepare analysis prompt
            main_content = content_data.get("text", "")
            tables_info = ""
            
            if content_data.get("tables"):
                tables_info = f"\n\n–ò–ó–í–õ–ï–ß–ï–ù–ù–´–ï –¢–ê–ë–õ–ò–¶–´ ({len(content_data['tables'])} —à—Ç.):\n"
                for table in content_data["tables"][:3]:  # Limit to first 3 tables
                    tables_info += f"–¢–∞–±–ª–∏—Ü–∞ {table.get('table_id', 'unknown')} ({table.get('row_count', 0)} —Å—Ç—Ä–æ–∫):\n"
                    for row in table.get("data", [])[:5]:  # First 5 rows
                        tables_info += " | ".join([str(cell)[:50] for cell in row]) + "\n"
                    tables_info += "\n"
            
            tz_comparison = ""
            if tz_content:
                tz_comparison = f"\n\n–¢–ï–•–ù–ò–ß–ï–°–ö–û–ï –ó–ê–î–ê–ù–ò–ï –î–õ–Ø –°–†–ê–í–ù–ï–ù–ò–Ø:\n{tz_content[:2000]}\n"
            
            full_prompt = f"""
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Å–ª–µ–¥—É—é—â–µ–µ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ:

–¢–ï–ö–°–¢ –ö–ü:
{main_content[:4000]}

{tables_info}

{tz_comparison}

–ò–ó–í–õ–ï–ß–ï–ù–ù–´–ï –í–ê–õ–Æ–¢–´: {len(content_data.get('currencies', []))} –Ω–∞–π–¥–µ–Ω–æ

–ü—Ä–æ–≤–µ–¥–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ –ø–æ 10 –∫—Ä–∏—Ç–µ—Ä–∏—è–º —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ –≤–µ—Å–∞–º–∏ –∏ –≤–µ—Ä–Ω–∏—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ç—Ä–æ–≥–æ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ.
"""
            
            response = await asyncio.create_task(
                asyncio.to_thread(
                    self.anthropic_client.messages.create,
                    model="claude-3-haiku-20240307",
                    max_tokens=4000,
                    temperature=0.1,  # Lower temperature for more consistent analysis
                    system=system_prompt,
                    messages=[{"role": "user", "content": full_prompt}]
                )
            )
            
            result_text = response.content[0].text
            logger.info(f"üéØ Claude v3 analysis response received (length: {len(result_text)})")
            
            # Try to parse as JSON
            try:
                return json.loads(result_text)
            except json.JSONDecodeError:
                logger.warning("‚ö†Ô∏è JSON parsing failed, using fallback analysis")
                return self._generate_fallback_v3_analysis(content_data, weights)
                
        except Exception as e:
            logger.error(f"‚ùå Claude v3 API error: {e}")
            logger.warning("üîÑ Using fallback v3 analysis system...")
            return self._generate_fallback_v3_analysis(content_data, weights)
    
    def _generate_fallback_v3_analysis(self, content_data: Dict[str, Any], weights: CriteriaWeight) -> Dict[str, Any]:
        """Generate comprehensive fallback analysis for v3"""
        text = content_data.get("text", "")
        tables = content_data.get("tables", [])
        currencies = content_data.get("currencies", [])
        
        # Enhanced analysis based on content
        company_name = "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–∞—è –∫–æ–º–ø–∞–Ω–∏—è"
        
        # Extract company name
        import re
        company_patterns = [
            r'(?:–∫–æ–º–ø–∞–Ω–∏—è|–û–û–û|–ò–ü|–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è)\s+["\']?([^"\'\n\r]+)["\']?',
            r'([–ê-–Ø–Å][–∞-—è—ë]+(?:\s+[–ê-–Ø–Å][–∞-—è—ë]+)*)\s+(?:–ø—Ä–µ–¥–ª–∞–≥–∞–µ—Ç|–≤—ã–ø–æ–ª–Ω–∏—Ç|—Ä–∞–∑—Ä–∞–±–æ—Ç–∞–µ—Ç)',
        ]
        
        for pattern in company_patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                company_name = match.group(1).strip()[:100]
                break
        
        # Generate detailed scores based on content analysis
        business_analysis = self._generate_detailed_criteria_analysis(text, tables, currencies, weights)
        
        # Calculate weighted score
        total_score = 0
        total_weight = 0
        
        for criterion, data in business_analysis.items():
            score = data["score"]
            weight = data["weight"]
            total_score += score * weight
            total_weight += weight
        
        weighted_score = total_score / total_weight if total_weight > 0 else 0
        overall_score = int(weighted_score)
        
        return {
            "company_name": company_name,
            "overall_score": overall_score,
            "weighted_score": round(weighted_score, 2),
            "executive_summary": f"–ü—Ä–æ–≤–µ–¥–µ–Ω —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ 10 –∫—Ä–∏—Ç–µ—Ä–∏—è–º. –û–±—â–∞—è –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–æ—Å—Ç–∞–≤–ª—è–µ—Ç {weighted_score:.1f}/100. –ù–∞–∏–±–æ–ª—å—à–∏–µ –≤–µ—Å–∞ –∏–º–µ—é—Ç –∫—Ä–∏—Ç–µ—Ä–∏–∏: —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ ({weights.technical_compliance:.1%}), —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ ({weights.functional_coverage:.1%}), –∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –±—é–¥–∂–µ—Ç—É ({weights.budget_compliance:.1%}).",
            "business_analysis": business_analysis,
            "recommendations": self._generate_v3_recommendations(business_analysis, overall_score),
            "risk_level": self._assess_risk_level(overall_score)
        }
    
    def _generate_detailed_criteria_analysis(self, text: str, tables: List, currencies: List, weights: CriteriaWeight) -> Dict[str, Dict]:
        """Generate detailed analysis for each criterion"""
        text_lower = text.lower()
        
        # Content-based scoring with keyword analysis
        budget_keywords = ['–±—é–¥–∂–µ—Ç', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ü–µ–Ω–∞', '—Ñ–∏–Ω–∞–Ω—Å—ã', '–æ–ø–ª–∞—Ç–∞', '—Å—É–º–º–∞']
        timeline_keywords = ['—Å—Ä–æ–∫', '–≤—Ä–µ–º—è', '–¥–∞—Ç–∞', '—ç—Ç–∞–ø', '–ø–ª–∞–Ω', '–≥—Ä–∞—Ñ–∏–∫']
        technical_keywords = ['—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è', '–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞', 'api', '–±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö', '—Å–µ—Ä–≤–µ—Ä', '–∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è']
        team_keywords = ['–∫–æ–º–∞–Ω–¥–∞', '—Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç', '–æ–ø—ã—Ç', '—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫', '—ç–∫—Å–ø–µ—Ä—Ç', '—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç']
        functional_keywords = ['—Ñ—É–Ω–∫—Ü–∏—è', '–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å', '–º–æ–¥—É–ª—å', '–∫–æ–º–ø–æ–Ω–µ–Ω—Ç', '–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å', '—Å–∏—Å—Ç–µ–º–∞']
        quality_keywords = ['–∫–∞—á–µ—Å—Ç–≤–æ', '—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ', '–∫–æ–Ω—Ç—Ä–æ–ª—å', '–ø—Ä–æ–≤–µ—Ä–∫–∞', '—Å—Ç–∞–Ω–¥–∞—Ä—Ç']
        methodology_keywords = ['agile', 'scrum', 'waterfall', '–º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è', '–ø—Ä–æ—Ü–µ—Å—Å', '–ø–æ–¥—Ö–æ–¥']
        scalability_keywords = ['–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ', '—Ä–æ—Å—Ç', '—Ä–∞–∑–≤–∏—Ç–∏–µ', '—Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ', '–ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å']
        communication_keywords = ['—Å–≤—è–∑—å', '–æ—Ç—á–µ—Ç', '–≤—Å—Ç—Ä–µ—á–∞', '–∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è', '–æ–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å']
        value_keywords = ['–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ', '–≤—ã–≥–æ–¥–∞', '–±–æ–Ω—É—Å', '–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ', '–∏–Ω–Ω–æ–≤–∞—Ü–∏—è']
        
        def calculate_keyword_score(keywords: List[str], base_score: int = 70) -> int:
            """Calculate score based on keyword presence"""
            matches = sum(1 for keyword in keywords if keyword in text_lower)
            boost = min(matches * 5, 25)  # Max 25 points boost
            return min(base_score + boost, 100)
        
        def get_compliance_level(score: int) -> str:
            """Get compliance level based on score"""
            if score >= 85: return "high"
            elif score >= 65: return "medium"
            else: return "low"
        
        # Analyze each criterion
        criteria_analysis = {}
        
        # Budget Compliance
        budget_score = calculate_keyword_score(budget_keywords, 75)
        if currencies:
            budget_score = min(budget_score + 10, 100)  # Bonus for currency detection
        if tables:
            budget_score = min(budget_score + 5, 100)   # Bonus for structured data
            
        criteria_analysis["budget_compliance"] = {
            "score": budget_score,
            "weight": weights.budget_compliance,
            "details": f"–ê–Ω–∞–ª–∏–∑ –±—é–¥–∂–µ—Ç–Ω–æ–≥–æ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –≤—ã—è–≤–∏–ª {len(currencies)} –≤–∞–ª—é—Ç–Ω—ã—Ö –ø–æ–∑–∏—Ü–∏–π –∏ {len([t for t in tables if '—Å—Ç–æ–∏–º–æ—Å—Ç—å' in str(t.get('data', [])).lower()])} —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ç–∞–±–ª–∏—Ü. –£—Ä–æ–≤–µ–Ω—å –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è –∫–∞–∫ {'–≤—ã—Å–æ–∫–∏–π' if budget_score >= 85 else '—Å—Ä–µ–¥–Ω–∏–π' if budget_score >= 65 else '–±–∞–∑–æ–≤—ã–π'}.",
            "recommendations": [
                "–î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞—Ç—Ä–∞—Ç –ø–æ —ç—Ç–∞–ø–∞–º –ø—Ä–æ–µ–∫—Ç–∞" if budget_score < 80 else "–ë—é–¥–∂–µ—Ç–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –ø–æ–¥—Ä–æ–±–Ω–æ",
                "–£–∫–∞–∑–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã –∏ —Ä–µ–∑–µ—Ä–≤—ã" if budget_score < 85 else "–§–∏–Ω–∞–Ω—Å–æ–≤–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—ã–≥–ª—è–¥–∏—Ç —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ"
            ],
            "compliance_level": get_compliance_level(budget_score),
            "risk_factors": [
                "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –¥–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –±—é–¥–∂–µ—Ç–∞" if budget_score < 70 else "–§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ —Ä–∏—Å–∫–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω—ã",
                "–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ —Ä–µ–∑–µ—Ä–≤–æ–≤ –Ω–∞ –Ω–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω—ã–µ —Ä–∞—Å—Ö–æ–¥—ã" if budget_score < 75 else ""
            ]
        }
        
        # Timeline Compliance
        timeline_score = calculate_keyword_score(timeline_keywords, 78)
        criteria_analysis["timeline_compliance"] = {
            "score": timeline_score,
            "weight": weights.timeline_compliance,
            "details": f"–í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞ {'—á–µ—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã' if timeline_score >= 80 else '—Ç—Ä–µ–±—É—é—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è'}. –û–±–Ω–∞—Ä—É–∂–µ–Ω—ã —É–ø–æ–º–∏–Ω–∞–Ω–∏—è —Å—Ä–æ–∫–æ–≤ –∏ —ç—Ç–∞–ø–æ–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏.",
            "recommendations": [
                "–î–æ–±–∞–≤–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ã–π –ø–ª–∞–Ω –ø—Ä–æ–µ–∫—Ç–∞" if timeline_score < 85 else "–í—Ä–µ–º–µ–Ω–Ω–æ–µ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ–µ",
                "–ü—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ—Ç—å –±—É—Ñ–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —ç—Ç–∞–ø–æ–≤"
            ],
            "compliance_level": get_compliance_level(timeline_score),
            "risk_factors": [
                "–°–∂–∞—Ç—ã–µ —Å—Ä–æ–∫–∏ –º–æ–≥—É—Ç –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ" if timeline_score < 70 else "",
                "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ" if timeline_score < 75 else ""
            ]
        }
        
        # Technical Compliance
        tech_score = calculate_keyword_score(technical_keywords, 80)
        criteria_analysis["technical_compliance"] = {
            "score": tech_score,
            "weight": weights.technical_compliance,
            "details": f"–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏—è {'–ø–æ–¥—Ä–æ–±–Ω–∞—è' if tech_score >= 85 else '–±–∞–∑–æ–≤–∞—è'}. –£–ø–æ–º—è–Ω—É—Ç—ã —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –ø–æ–¥—Ö–æ–¥—ã –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ.",
            "recommendations": [
                "–î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ —Ä–µ—à–µ–Ω–∏—è" if tech_score < 80 else "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ—Ä–∞–±–æ—Ç–∞–Ω–æ",
                "–£–∫–∞–∑–∞—Ç—å –ø–ª–∞–Ω –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏"
            ],
            "compliance_level": get_compliance_level(tech_score),
            "risk_factors": [
                "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ä–∏—Å–∫–∏ —Ç—Ä–µ–±—É—é—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –ø—Ä–æ—Ä–∞–±–æ—Ç–∫–∏" if tech_score < 70 else "",
                "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Ç–æ—á–Ω–∏—Ç—å —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–π"
            ]
        }
        
        # Team Expertise
        team_score = calculate_keyword_score(team_keywords, 72)
        criteria_analysis["team_expertise"] = {
            "score": team_score,
            "weight": weights.team_expertise,
            "details": f"–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–º–∞–Ω–¥–µ {'–ø–æ–¥—Ä–æ–±–Ω–∞—è' if team_score >= 80 else '–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è'}. –£–∫–∞–∑–∞–Ω –æ–ø—ã—Ç –∏ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤.",
            "recommendations": [
                "–ü—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Ä–µ–∑—é–º–µ –∫–ª—é—á–µ–≤—ã—Ö —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –ø—Ä–æ–µ–∫—Ç–∞" if team_score < 75 else "–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–æ–º–∞–Ω–¥–µ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è",
                "–£–∫–∞–∑–∞—Ç—å –ø–ª–∞–Ω–∏—Ä—É–µ–º—É—é –∑–∞–≥—Ä—É–∑–∫—É —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤ –ø–æ –ø—Ä–æ–µ–∫—Ç—É"
            ],
            "compliance_level": get_compliance_level(team_score),
            "risk_factors": [
                "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∫–æ–º–∞–Ω–¥—ã" if team_score < 70 else "",
                "–†–∏—Å–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç–æ–≤"
            ]
        }
        
        # Functional Coverage
        functional_score = calculate_keyword_score(functional_keywords, 76)
        criteria_analysis["functional_coverage"] = {
            "score": functional_score,
            "weight": weights.functional_coverage,
            "details": f"–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è {'–ø–æ–ª–Ω–æ—Å—Ç—å—é –ø–æ–∫—Ä—ã—Ç—ã' if functional_score >= 85 else '—á–∞—Å—Ç–∏—á–Ω–æ –æ–ø–∏—Å–∞–Ω—ã'}. –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –æ—Å–Ω–æ–≤–Ω—ã–µ –º–æ–¥—É–ª–∏ –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–∏—Å—Ç–µ–º—ã.",
            "recommendations": [
                "–î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏" if functional_score < 80 else "–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –æ–ø–∏—Å–∞–Ω–∞ –∞–¥–µ–∫–≤–∞—Ç–Ω–æ",
                "–î–æ–±–∞–≤–∏—Ç—å —Å—Ö–µ–º—ã –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–æ–≤ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"
            ],
            "compliance_level": get_compliance_level(functional_score),
            "risk_factors": [
                "–ù–µ–ø–æ–ª–Ω–æ–µ –ø–æ–∫—Ä—ã—Ç–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π" if functional_score < 70 else "",
                "–í–æ–∑–º–æ–∂–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏"
            ]
        }
        
        # Quality Assurance
        qa_score = calculate_keyword_score(quality_keywords, 70)
        criteria_analysis["quality_assurance"] = {
            "score": qa_score,
            "weight": weights.quality_assurance,
            "details": f"–ü—Ä–æ—Ü–µ—Å—Å—ã –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ {'—Ö–æ—Ä–æ—à–æ –ø—Ä–æ—Ä–∞–±–æ—Ç–∞–Ω—ã' if qa_score >= 80 else '—Ç—Ä–µ–±—É—é—Ç —É—Å–∏–ª–µ–Ω–∏—è'}. –£–ø–æ–º—è–Ω—É—Ç—ã –º–µ—Ç–æ–¥—ã —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –∫–æ–Ω—Ç—Ä–æ–ª—è.",
            "recommendations": [
                "–î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–ª–∞–Ω —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è" if qa_score < 75 else "–ü–æ–¥—Ö–æ–¥ –∫ –∫–æ–Ω—Ç—Ä–æ–ª—é –∫–∞—á–µ—Å—Ç–≤–∞ –∞–¥–µ–∫–≤–∞—Ç–Ω—ã–π",
                "–£–∫–∞–∑–∞—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏–µ–º–∫–∏ –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ"
            ],
            "compliance_level": get_compliance_level(qa_score),
            "risk_factors": [
                "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –∫–æ–Ω—Ç—Ä–æ–ª—é –∫–∞—á–µ—Å—Ç–≤–∞" if qa_score < 65 else "",
                "–†–∏—Å–∫ –≤—ã—è–≤–ª–µ–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç–æ–≤ –Ω–∞ –ø–æ–∑–¥–Ω–∏—Ö —ç—Ç–∞–ø–∞—Ö"
            ]
        }
        
        # Development Methodology
        method_score = calculate_keyword_score(methodology_keywords, 68)
        criteria_analysis["development_methodology"] = {
            "score": method_score,
            "weight": weights.development_methodology,
            "details": f"–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ {'—á–µ—Ç–∫–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞' if method_score >= 80 else '—Ç—Ä–µ–±—É–µ—Ç —É—Ç–æ—á–Ω–µ–Ω–∏—è'}. –û–ø–∏—Å–∞–Ω –ø–æ–¥—Ö–æ–¥ –∫ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—é –ø—Ä–æ–µ–∫—Ç–æ–º.",
            "recommendations": [
                "–£–∫–∞–∑–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏—é —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–æ–º" if method_score < 75 else "–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∞–¥–µ–∫–≤–∞—Ç–Ω–∞—è",
                "–î–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ü–µ—Å—Å—ã –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏ —Å –∑–∞–∫–∞–∑—á–∏–∫–æ–º"
            ],
            "compliance_level": get_compliance_level(method_score),
            "risk_factors": [
                "–ù–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –≤ –ø—Ä–æ—Ü–µ—Å—Å–∞—Ö —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏" if method_score < 65 else "",
                "–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∫–æ–æ—Ä–¥–∏–Ω–∞—Ü–∏–∏ –∫–æ–º–∞–Ω–¥—ã"
            ]
        }
        
        # Scalability
        scalability_score = calculate_keyword_score(scalability_keywords, 74)
        criteria_analysis["scalability"] = {
            "score": scalability_score,
            "weight": weights.scalability,
            "details": f"–í–æ–ø—Ä–æ—Å—ã –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ {'—É—á—Ç–µ–Ω—ã' if scalability_score >= 80 else '—Å–ª–∞–±–æ –ø—Ä–æ—Ä–∞–±–æ—Ç–∞–Ω—ã'}. –†–∞—Å—Å–º–æ—Ç—Ä–µ–Ω—ã –∞—Å–ø–µ–∫—Ç—ã —Ä–∞–∑–≤–∏—Ç–∏—è —Å–∏—Å—Ç–µ–º—ã.",
            "recommendations": [
                "–û–ø–∏—Å–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –¥–ª—è –±—É–¥—É—â–µ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è" if scalability_score < 80 else "–ü–æ–¥—Ö–æ–¥ –∫ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—é –æ–±–æ—Å–Ω–æ–≤–∞–Ω",
                "–ü—Ä–µ–¥—É—Å–º–æ—Ç—Ä–µ—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥—É–ª–µ–π"
            ],
            "compliance_level": get_compliance_level(scalability_score),
            "risk_factors": [
                "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Ä–∞–∑–≤–∏—Ç–∏—è —Å–∏—Å—Ç–µ–º—ã" if scalability_score < 70 else "",
                "–ù–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –¥–æ—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏ —Ä–æ—Å—Ç–µ –Ω–∞–≥—Ä—É–∑–∫–∏"
            ]
        }
        
        # Communication
        comm_score = calculate_keyword_score(communication_keywords, 71)
        criteria_analysis["communication"] = {
            "score": comm_score,
            "weight": weights.communication,
            "details": f"–ü–ª–∞–Ω –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π {'–¥–µ—Ç–∞–ª—å–Ω–æ –ø—Ä–æ—Ä–∞–±–æ—Ç–∞–Ω' if comm_score >= 80 else '–±–∞–∑–æ–≤—ã–π'}. –û–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –ø—Ä–æ—Ü–µ—Å—Å—ã –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –∑–∞–∫–∞–∑—á–∏–∫–æ–º.",
            "recommendations": [
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å —Ä–µ–≥—É–ª—è—Ä–Ω–æ—Å—Ç—å –æ—Ç—á–µ—Ç–æ–≤ –∏ –≤—Å—Ç—Ä–µ—á" if comm_score < 75 else "–ö–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–æ–Ω–Ω—ã–π –ø–ª–∞–Ω –∞–¥–µ–∫–≤–∞—Ç–Ω—ã–π",
                "–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∫–∞–Ω–∞–ª—ã —ç–∫—Å—Ç—Ä–µ–Ω–Ω–æ–π —Å–≤—è–∑–∏"
            ],
            "compliance_level": get_compliance_level(comm_score),
            "risk_factors": [
                "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –ø—Ä–æ—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏" if comm_score < 65 else "",
                "–í–æ–∑–º–æ–∂–Ω—ã–µ –Ω–µ–¥–æ–ø–æ–Ω–∏–º–∞–Ω–∏—è —Å –∑–∞–∫–∞–∑—á–∏–∫–æ–º"
            ]
        }
        
        # Added Value
        value_score = calculate_keyword_score(value_keywords, 69)
        criteria_analysis["added_value"] = {
            "score": value_score,
            "weight": weights.added_value,
            "details": f"–î–æ–±–∞–≤–ª–µ–Ω–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å {'–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–∞—è' if value_score >= 80 else '—É–º–µ—Ä–µ–Ω–Ω–∞—è'}. –ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏.",
            "recommendations": [
                "–í—ã–¥–µ–ª–∏—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è" if value_score < 75 else "–î–æ–±–∞–≤–ª–µ–Ω–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å –æ—á–µ–≤–∏–¥–Ω–∞",
                "–ü–æ–∫–∞–∑–∞—Ç—å –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω—ã–µ –≤—ã–≥–æ–¥—ã –¥–ª—è –∑–∞–∫–∞–∑—á–∏–∫–∞"
            ],
            "compliance_level": get_compliance_level(value_score),
            "risk_factors": [
                "–û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –¥–æ–±–∞–≤–ª–µ–Ω–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å" if value_score < 65 else "",
                "–ö–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –º–æ–≥—É—Ç –±—ã—Ç—å –±–æ–ª–µ–µ –ø—Ä–∏–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–º–∏"
            ]
        }
        
        return criteria_analysis
    
    def _generate_v3_recommendations(self, business_analysis: Dict, overall_score: int) -> List[str]:
        """Generate comprehensive recommendations for v3"""
        recommendations = []
        
        if overall_score >= 85:
            recommendations.append("üéØ –ö–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫ –ø—Ä–∏–Ω—è—Ç–∏—é")
            recommendations.append("‚úÖ –í—Å–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç –≤—ã—Å–æ–∫–∏–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞–º")
        elif overall_score >= 70:
            recommendations.append("‚ö° –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è")
            recommendations.append("üîß –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–æ—Ä–∞–±–æ—Ç–∫–∞ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º")
        elif overall_score >= 55:
            recommendations.append("‚ö†Ô∏è –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ —Ç—Ä–µ–±—É–µ—Ç —Å—É—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –¥–æ—Ä–∞–±–æ—Ç–∫–∏")
            recommendations.append("üìù –ù–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å–∏–ª–∏—Ç—å —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã –ø–µ—Ä–µ–¥ –ø—Ä–∏–Ω—è—Ç–∏–µ–º")
        else:
            recommendations.append("‚ùå –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º")
            recommendations.append("üîÑ –¢—Ä–µ–±—É–µ—Ç—Å—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞")
        
        # Add specific recommendations based on low-scoring criteria
        for criterion, data in business_analysis.items():
            if data["score"] < 65:
                if criterion == "budget_compliance":
                    recommendations.append("üí∞ –ö—Ä–∏—Ç–∏—á–Ω–æ: –ø–µ—Ä–µ—Å–º–æ—Ç—Ä–µ—Ç—å –±—é–¥–∂–µ—Ç–Ω—É—é —á–∞—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è")
                elif criterion == "technical_compliance":
                    recommendations.append("üîß –ö—Ä–∏—Ç–∏—á–Ω–æ: —É—Å–∏–ª–∏—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é –ø—Ä–æ—Ä–∞–±–æ—Ç–∫—É —Ä–µ—à–µ–Ω–∏—è")
                elif criterion == "functional_coverage":
                    recommendations.append("üìã –ö—Ä–∏—Ç–∏—á–Ω–æ: –¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è")
        
        # Add best practices recommendations
        if overall_score < 80:
            recommendations.append("üìä –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Å—Ç–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∏—Å–∫–æ–≤")
            recommendations.append("ü§ù –û—Ä–≥–∞–Ω–∏–∑–æ–≤–∞—Ç—å –≤—Å—Ç—Ä–µ—á—É —Å –∫–æ–º–∞–Ω–¥–æ–π –∏—Å–ø–æ–ª–Ω–∏—Ç–µ–ª–µ–π")
        
        return recommendations[:6]  # Limit to 6 recommendations

# Initialize AI Provider Manager
ai_manager = AIProviderManager()

# Initialize Authentication Components
auth_security = HTTPBearer()

# Create directories
os.makedirs("data/reports", exist_ok=True)
os.makedirs("data/uploads", exist_ok=True)

# Authentication Helper Functions
def create_user(email: str, password: str, full_name: str) -> Dict[str, Any]:
    """Create a new user (simplified version for demo)"""
    global user_id_counter
    
    # Check if user already exists
    for user in users_storage.values():
        if user["email"] == email.lower():
            raise HTTPException(status_code=400, detail="Email already registered")
    
    # Validate password
    is_valid, errors = simple_password_manager.validate_password(password)
    if not is_valid:
        raise HTTPException(status_code=400, detail=f"Password validation failed: {', '.join(errors)}")
    
    # Hash password
    hashed_password = simple_password_manager.hash_password(password)
    
    # Create user
    user_id = user_id_counter
    user_id_counter += 1
    
    user_data = {
        "id": user_id,
        "email": email.lower(),
        "hashed_password": hashed_password,
        "full_name": full_name,
        "is_active": True,
        "created_at": datetime.now().isoformat()
    }
    
    users_storage[user_id] = user_data
    logger.info(f"User created: {email} (ID: {user_id})")
    
    return user_data

def authenticate_user(email: str, password: str) -> Optional[Dict[str, Any]]:
    """Authenticate user credentials"""
    email = email.lower()
    
    # Find user by email
    user = None
    for user_data in users_storage.values():
        if user_data["email"] == email:
            user = user_data
            break
    
    if not user:
        return None
    
    if not user["is_active"]:
        raise HTTPException(status_code=400, detail="Inactive user")
    
    # Verify password
    if not simple_password_manager.verify_password(password, user["hashed_password"]):
        return None
    
    return user

def get_user_by_id(user_id: int) -> Optional[Dict[str, Any]]:
    """Get user by ID"""
    return users_storage.get(user_id)

def get_current_user_simple(credentials: HTTPAuthorizationCredentials = Depends(auth_security)) -> Dict[str, Any]:
    """Simple dependency to get current user from JWT token"""
    try:
        # Verify JWT token
        payload = simple_jwt_manager.verify_token(credentials.credentials)
        user_id = payload.get("user_id")
        
        if not user_id:
            raise HTTPException(status_code=401, detail="Invalid token payload")
        
        # Get user data
        user = get_user_by_id(user_id)
        if not user:
            raise HTTPException(status_code=401, detail="User not found")
        
        if not user["is_active"]:
            raise HTTPException(status_code=401, detail="Inactive user")
        
        return user
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Authentication error: {e}")
        raise HTTPException(status_code=401, detail="Could not validate credentials")

@app.get("/")
async def root():
    """Root endpoint"""
    return {
        "message": "DevAssist Pro API", 
        "status": "running", 
        "version": "3.0.0",
        "features": [
            "KP Analyzer v2 - –ë–∞–∑–æ–≤–∞—è –≤–µ—Ä—Å–∏—è",
            "KP Analyzer v3 - –≠–∫—Å–ø–µ—Ä—Ç–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å 10 –∫—Ä–∏—Ç–µ—Ä–∏—è–º–∏",
            "Advanced PDF Processing",
            "Multi-currency Support",
            "Professional PDF Export"
        ]
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    return HealthResponse(
        status="healthy",
        timestamp=datetime.now().isoformat(),
        services={
            "api": "running",
            "documents": "running", 
            "llm": "available",
            "reports": "running",
            "auth": "available"
        }
    )

# ============================================================================
# AUTHENTICATION ENDPOINTS
# ============================================================================

@app.post("/api/auth/register", response_model=AuthResponse)
async def register_user(user_data: UserCreate):
    """Register a new user"""
    try:
        logger.info(f"Registration attempt for email: {user_data.email}")
        
        # Create user
        user = create_user(user_data.email, user_data.password, user_data.full_name)
        
        # Generate tokens
        token_data = {"user_id": user["id"], "email": user["email"]}
        access_token = simple_jwt_manager.create_access_token(token_data)
        refresh_token = simple_jwt_manager.create_refresh_token(user["id"])
        
        # Create response
        user_response = UserResponse(
            id=user["id"],
            email=user["email"],
            full_name=user["full_name"],
            is_active=user["is_active"],
            created_at=user["created_at"]
        )
        
        logger.info(f"User registered successfully: {user['email']} (ID: {user['id']})")
        
        return AuthResponse(
            access_token=access_token,
            refresh_token=refresh_token,
            token_type="bearer",
            user=user_response
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Registration error: {e}")
        raise HTTPException(status_code=500, detail="Registration failed")

@app.post("/api/auth/login", response_model=AuthResponse)
async def login_user(user_credentials: UserLogin):
    """Login user"""
    try:
        logger.info(f"Login attempt for email: {user_credentials.email}")
        
        # Authenticate user
        user = authenticate_user(user_credentials.email, user_credentials.password)
        
        if not user:
            logger.warning(f"Failed login attempt for: {user_credentials.email}")
            raise HTTPException(
                status_code=401, 
                detail="Incorrect email or password"
            )
        
        # Generate tokens
        token_data = {"user_id": user["id"], "email": user["email"]}
        access_token = simple_jwt_manager.create_access_token(token_data)
        refresh_token = simple_jwt_manager.create_refresh_token(user["id"])
        
        # Create response
        user_response = UserResponse(
            id=user["id"],
            email=user["email"],
            full_name=user["full_name"],
            is_active=user["is_active"],
            created_at=user["created_at"]
        )
        
        logger.info(f"User logged in successfully: {user['email']} (ID: {user['id']})")
        
        return AuthResponse(
            access_token=access_token,
            refresh_token=refresh_token,
            token_type="bearer",
            user=user_response
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Login error: {e}")
        raise HTTPException(status_code=500, detail="Login failed")

@app.get("/api/auth/me", response_model=UserResponse)
async def get_current_user_info(current_user: Dict[str, Any] = Depends(get_current_user_simple)):
    """Get current user information"""
    return UserResponse(
        id=current_user["id"],
        email=current_user["email"],
        full_name=current_user["full_name"],
        is_active=current_user["is_active"],
        created_at=current_user["created_at"]
    )

@app.post("/api/auth/refresh")
async def refresh_access_token(refresh_data: TokenRefresh):
    """Refresh access token using refresh token"""
    try:
        # Verify refresh token
        payload = simple_jwt_manager.verify_token(refresh_data.refresh_token, token_type="refresh")
        user_id = payload.get("user_id")
        
        if not user_id:
            raise HTTPException(status_code=401, detail="Invalid refresh token")
        
        # Get user data
        user = get_user_by_id(user_id)
        if not user:
            raise HTTPException(status_code=401, detail="User not found")
        
        if not user["is_active"]:
            raise HTTPException(status_code=401, detail="Inactive user")
        
        # Generate new access token
        token_data = {"user_id": user["id"], "email": user["email"]}
        new_access_token = simple_jwt_manager.create_access_token(token_data)
        
        logger.info(f"Token refreshed for user: {user['email']} (ID: {user['id']})")
        
        return {
            "access_token": new_access_token,
            "token_type": "bearer"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Token refresh error: {e}")
        raise HTTPException(status_code=401, detail="Token refresh failed")

@app.post("/api/auth/logout")
async def logout_user(current_user: Dict[str, Any] = Depends(get_current_user_simple)):
    """Logout user (for future implementation of token blacklisting)"""
    logger.info(f"User logged out: {current_user['email']} (ID: {current_user['id']})")
    return {"message": "Successfully logged out"}

def extract_text_from_file(file_content: bytes, filename: str, content_type: str) -> str:
    """Enhanced document text extraction"""
    try:
        logger.info(f"üìÑ Extracting text from {filename} ({content_type})")
        
        if content_type == "application/pdf" or filename.lower().endswith('.pdf'):
            # PDF extraction using PyPDF2
            from io import BytesIO
            pdf_reader = PyPDF2.PdfReader(BytesIO(file_content))
            text = ""
            for page in pdf_reader.pages:
                text += page.extract_text() + "\n"
            return text.strip()
            
        elif content_type == "application/vnd.openxmlformats-officedocument.wordprocessingml.document" or filename.lower().endswith('.docx'):
            # DOCX extraction
            from io import BytesIO
            doc = docx.Document(BytesIO(file_content))
            text = ""
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            return text.strip()
            
        elif content_type == "text/plain" or filename.lower().endswith('.txt'):
            # Plain text
            return file_content.decode('utf-8')
            
        else:
            # Try to decode as text
            return file_content.decode('utf-8')
            
    except Exception as e:
        logger.error(f"‚ùå Text extraction error for {filename}: {e}")
        return f"–û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –∏–∑ {filename}. –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {len(file_content)} –±–∞–π—Ç."

@app.post("/api/documents/upload")
async def upload_document(file: UploadFile = File(...)):
    """Enhanced document upload with text extraction"""
    try:
        doc_id = len(document_storage) + 1
        
        # Read file content
        file_content = await file.read()
        
        # Extract text content
        extracted_text = extract_text_from_file(file_content, file.filename, file.content_type)
        
        # Store file info and content
        document_storage[doc_id] = {
            "id": doc_id,
            "filename": file.filename,
            "content_type": file.content_type,
            "status": "uploaded",
            "size": len(file_content),
            "created_at": datetime.now().isoformat()
        }
        
        # Store extracted text
        file_content_storage[doc_id] = {
            "raw_content": file_content,
            "extracted_text": extracted_text,
            "text_length": len(extracted_text)
        }
        
        logger.info(f"‚úÖ Document uploaded: {file.filename} (ID: {doc_id}, Size: {len(file_content)} bytes, Text: {len(extracted_text)} chars)")
        
        return {
            "id": doc_id,
            "filename": file.filename,
            "status": "uploaded",
            "message": "Document uploaded successfully",
            "text_extracted": len(extracted_text) > 0,
            "text_length": len(extracted_text)
        }
        
    except Exception as e:
        logger.error(f"‚ùå Upload error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/documents/{document_id}/analyze")
async def analyze_document(document_id: int):
    """Enhanced document analysis with real Claude AI integration"""
    start_time = datetime.now()
    
    try:
        if document_id not in document_storage:
            raise HTTPException(status_code=404, detail="Document not found")
        
        if document_id not in file_content_storage:
            raise HTTPException(status_code=404, detail="Document content not found")
        
        document_info = document_storage[document_id]
        content_data = file_content_storage[document_id]
        extracted_text = content_data["extracted_text"]
        
        logger.info(f"üöÄ Starting enhanced analysis for document {document_id} ({document_info['filename']})")
        
        if not extracted_text or len(extracted_text.strip()) == 0:
            raise HTTPException(status_code=400, detail="No text content extracted from document")
        
        # Enhanced AI analysis prompt
        analysis_prompt = """
–ü—Ä–æ–≤–µ–¥–∏—Ç–µ –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω–æ–≥–æ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è.

–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –æ—Ü–µ–Ω–∏—Ç—å –ö–ü –ø–æ 10 –∫—Ä–∏—Ç–µ—Ä–∏—è–º –∏ –≤–µ—Ä–Ω—É—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
{
    "company_name": "–Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ —Ç–µ–∫—Å—Ç–∞",
    "overall_score": –æ–±—â–∞—è –æ—Ü–µ–Ω–∫–∞ (0-100),
    "analysis_type": "enhanced",
    "summary": "–∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –∞–Ω–∞–ª–∏–∑–∞",
    "business_analysis": {
        "technical_compliance": {"score": 0-100, "weight": 0.3, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ"},
        "functional_completeness": {"score": 0-100, "weight": 0.3, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ"},
        "economic_efficiency": {"score": 0-100, "weight": 0.2, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ"},
        "timeline_realism": {"score": 0-100, "weight": 0.1, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ"},
        "vendor_reliability": {"score": 0-100, "weight": 0.1, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ"},
        "risk_assessment": {"score": 0-100, "weight": 0.0, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ"},
        "competitiveness": {"score": 0-100, "weight": 0.0, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ"},
        "compliance_evaluation": {"score": 0-100, "weight": 0.0, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ"},
        "innovation_assessment": {"score": 0-100, "weight": 0.0, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ"},
        "sustainability_factors": {"score": 0-100, "weight": 0.0, "details": "–æ–ø–∏—Å–∞–Ω–∏–µ"}
    },
    "risk_level": "–ù–∏–∑–∫–∏–π/–£–º–µ—Ä–µ–Ω–Ω—ã–π/–°—Ä–µ–¥–Ω–∏–π/–í—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫",
    "recommendations": ["—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 1", "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 2", "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 3"]
}
        """
        
        # Use real Claude API or enhanced fallback
        try:
            logger.info("ü§ñ Calling Claude API for analysis...")
            analysis_data = await ai_manager.analyze_with_claude(
                prompt=analysis_prompt,
                content=extracted_text[:8000],  # Limit content length
                model="claude-3-haiku-20240307"
            )
            
            logger.info("‚úÖ Claude API analysis completed successfully")
            
        except Exception as api_error:
            logger.warning(f"‚ö†Ô∏è Claude API failed: {api_error}")
            logger.info("üîÑ Using enhanced fallback analysis system...")
            
            # Enhanced fallback analysis
            analysis_data = ai_manager._parse_structured_response(extracted_text)
        
        # Calculate processing time
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Prepare final analysis result
        analysis_result = {
            "id": document_id,
            "status": "completed",
            "overall_score": analysis_data.get("overall_score", 75),
            "company_name": analysis_data.get("company_name", "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–∞—è –∫–æ–º–ø–∞–Ω–∏—è"),
            "analysis_type": "enhanced",
            "summary": analysis_data.get("summary", f"–ü—Ä–æ–≤–µ–¥–µ–Ω –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å –æ–±—â–µ–π –æ—Ü–µ–Ω–∫–æ–π {analysis_data.get('overall_score', 75)}/100"),
            "recommendations": analysis_data.get("recommendations", [
                "–†–∞—Å—Å–º–æ—Ç—Ä–µ—Ç—å —Ç–µ—Ö–Ω–∏—á–µ—Å–∫—É—é —Ä–µ–∞–ª–∏–∑—É–µ–º–æ—Å—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è",
                "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –±—é–¥–∂–µ—Ç–Ω—ã–º –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è–º",
                "–£—Ç–æ—á–Ω–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–∞–º–∫–∏ –ø—Ä–æ–µ–∫—Ç–∞"
            ]),
            "business_analysis": analysis_data.get("business_analysis", {}),
            "risk_level": analysis_data.get("risk_level", "–£–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∏—Å–∫"),
            "created_at": datetime.now().isoformat(),
            "processing_time": processing_time,
            "document_filename": document_info["filename"],
            "text_length": len(extracted_text)
        }
        
        # Store analysis result
        analysis_storage[document_id] = analysis_result
        
        logger.info(f"üéØ Document analysis completed: {document_id} (Score: {analysis_result['overall_score']}/100, Time: {processing_time:.2f}s)")
        
        return analysis_result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Analysis error: {e}")
        logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")

@app.post("/api/documents/{document_id}/export-pdf")
async def export_pdf(document_id: int):
    """Enhanced PDF export with real PDF generation using Tender-style exporter"""
    try:
        if document_id not in analysis_storage:
            raise HTTPException(status_code=404, detail="Analysis not found")
        
        analysis_data = analysis_storage[document_id]
        
        logger.info(f"üîÑ Generating PDF for document {document_id} using Tender-style exporter...")
        
        # Generate PDF using the Tender-style exporter
        try:
            pdf_buffer = tender_pdf_exporter.generate_kp_analysis_pdf(analysis_data)
            
            # Generate filename
            company_name = analysis_data.get("company_name", "Unknown_Company").replace(" ", "_")
            pdf_filename = f"KP_Analysis_{company_name}_{document_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
            
            # Save PDF to reports directory
            pdf_path = Path(f"data/reports/{pdf_filename}")
            with open(pdf_path, 'wb') as f:
                f.write(pdf_buffer.read())
            
            logger.info(f"‚úÖ PDF generated successfully: {pdf_filename} (Size: {pdf_path.stat().st_size} bytes)")
            
            # Return PDF as streaming response
            pdf_buffer.seek(0)
            return StreamingResponse(
                BytesIO(pdf_buffer.read()),
                media_type="application/pdf",
                headers={"Content-Disposition": f"attachment; filename={pdf_filename}"}
            )
            
        except Exception as pdf_error:
            logger.error(f"‚ùå PDF generation error: {pdf_error}")
            logger.error(f"‚ùå PDF Traceback: {traceback.format_exc()}")
            
            # Fallback response
            pdf_filename = f"DevAssist_Pro_KP_Analysis_{document_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
            
            return {
                "message": "PDF generation failed - using fallback",
                "filename": pdf_filename,
                "document_id": document_id,
                "analysis_score": analysis_data.get("overall_score"),
                "export_time": datetime.now().isoformat(),
                "error": str(pdf_error),
                "fallback_mode": True
            }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå PDF export error: {e}")
        raise HTTPException(status_code=500, detail=f"PDF export failed: {str(e)}")

@app.post("/api/test/cyrillic-pdf")
async def test_cyrillic_pdf():
    """Test endpoint for Cyrillic PDF generation - creates test PDF with Russian text"""
    try:
        logger.info("üß™ –¢–ï–°–¢: –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π PDF —Å –∫–∏—Ä–∏–ª–ª–∏—Ü–µ–π...")
        
        # Generate test PDF with Cyrillic text
        pdf_buffer = tender_pdf_exporter.test_cyrillic_support()
        
        # Generate test filename
        test_filename = f"DevAssist_Pro_Cyrillic_Test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
        
        # Save test PDF to reports directory for inspection
        pdf_path = Path(f"data/reports/{test_filename}")
        pdf_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(pdf_path, 'wb') as f:
            f.write(pdf_buffer.read())
        
        logger.info(f"‚úÖ –¢–ï–°–¢: PDF —Å –∫–∏—Ä–∏–ª–ª–∏—Ü–µ–π —Å–æ–∑–¥–∞–Ω: {test_filename} (–†–∞–∑–º–µ—Ä: {pdf_path.stat().st_size} –±–∞–π—Ç)")
        
        # Return PDF as streaming response
        pdf_buffer.seek(0)
        return StreamingResponse(
            BytesIO(pdf_buffer.read()),
            media_type="application/pdf",
            headers={"Content-Disposition": f"attachment; filename={test_filename}"}
        )
        
    except Exception as e:
        logger.error(f"‚ùå –û–®–ò–ë–ö–ê —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∏—Ä–∏–ª–ª–∏—Ü—ã –≤ PDF: {e}")
        logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"Cyrillic PDF test failed: {str(e)}")

@app.get("/api/documents/{document_id}/analysis-status")
async def get_analysis_status(document_id: int):
    """Get analysis status"""
    try:
        if document_id in analysis_storage:
            return analysis_storage[document_id]
        elif document_id in document_storage:
            return {"status": "pending", "message": "Analysis not started"}
        else:
            raise HTTPException(status_code=404, detail="Document not found")
            
    except Exception as e:
        logger.error(f"Status check error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/llm/analyze")
async def analyze_with_llm(request: LLMRequest):
    """Direct LLM analysis endpoint"""
    start_time = datetime.now()
    
    try:
        logger.info(f"ü§ñ Direct LLM analysis request (Model: {request.model})")
        
        # Use Claude API for analysis
        analysis_result = await ai_manager.analyze_with_claude(
            prompt=request.prompt,
            content=request.content,
            model=request.model
        )
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"‚úÖ LLM analysis completed in {processing_time:.2f}s")
        
        return {
            "status": "completed",
            "result": analysis_result,
            "processing_time": processing_time,
            "model_used": request.model,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå LLM analysis error: {e}")
        raise HTTPException(status_code=500, detail=f"LLM analysis failed: {str(e)}")

@app.get("/api/llm/providers")
async def get_llm_providers():
    """Get LLM provider status"""
    return {
        "providers": {
            "anthropic": {
                "status": "available" if os.getenv('ANTHROPIC_API_KEY') else "not_configured",
                "models": ["claude-3-haiku-20240307", "claude-3-sonnet-20240229"],
                "client_initialized": ai_manager.anthropic_client is not None
            },
            "openai": {
                "status": "available" if os.getenv('OPENAI_API_KEY') else "not_configured", 
                "models": ["gpt-3.5-turbo", "gpt-4"],
                "client_initialized": ai_manager.openai_client is not None
            }
        },
        "default_provider": "anthropic",
        "fallback_enabled": True
    }

# ============================================================================
# KP ANALYZER V3 ENDPOINTS - EXPERT ANALYSIS WITH 10-CRITERIA SYSTEM
# ============================================================================

@app.get("/api/v3/test")
async def v3_test():
    """Simple V3 test endpoint"""
    return {"status": "V3 endpoints are working", "version": "3.0.0"}

@app.get("/api/v3/criteria/weights/presets")
async def get_weight_presets():
    """Get available criteria weight presets"""
    return {
        "presets": {
            "balanced": {
                "name": "–°–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π",
                "description": "–†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –ø–æ –≤—Å–µ–º –∫—Ä–∏—Ç–µ—Ä–∏—è–º",
                "weights": criteria_weights_presets["balanced"].dict()
            },
            "budget_focused": {
                "name": "–ë—é–¥–∂–µ—Ç–Ω–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π",
                "description": "–ü–æ–≤—ã—à–µ–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º –∞—Å–ø–µ–∫—Ç–∞–º",
                "weights": criteria_weights_presets["budget_focused"].dict()
            },
            "technical_focused": {
                "name": "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π", 
                "description": "–ê–∫—Ü–µ–Ω—Ç –Ω–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö —Ä–µ—à–µ–Ω–∏—è—Ö –∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏",
                "weights": criteria_weights_presets["technical_focused"].dict()
            },
            "quality_focused": {
                "name": "–ö–∞—á–µ—Å—Ç–≤–æ-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π",
                "description": "–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –∫–æ–Ω—Ç—Ä–æ–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏",
                "weights": criteria_weights_presets["quality_focused"].dict()
            }
        },
        "criteria_descriptions": {
            "budget_compliance": "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –±—é–¥–∂–µ—Ç–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º",
            "timeline_compliance": "–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ä–∞–º–∫–∞–º",
            "technical_compliance": "–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º",
            "team_expertise": "–≠–∫—Å–ø–µ—Ä—Ç–∏–∑–∞ –∏ –∫–≤–∞–ª–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ–º–∞–Ω–¥—ã",
            "functional_coverage": "–ü–æ–∫—Ä—ã—Ç–∏–µ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π",
            "quality_assurance": "–ü—Ä–æ—Ü–µ—Å—Å—ã –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞",
            "development_methodology": "–ú–µ—Ç–æ–¥–æ–ª–æ–≥–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏",
            "scalability": "–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è",
            "communication": "–ö–∞—á–µ—Å—Ç–≤–æ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–∏",
            "added_value": "–î–æ–±–∞–≤–ª–µ–Ω–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å"
        }
    }

@app.post("/api/v3/criteria/weights/custom")
async def set_custom_weights(weights: CriteriaWeight):
    """Set custom criteria weights"""
    # Validate that weights sum to 1.0
    total_weight = sum(weights.dict().values())
    if not (0.99 <= total_weight <= 1.01):  # Allow small floating point errors
        raise HTTPException(
            status_code=400, 
            detail=f"Weights must sum to 1.0, got {total_weight:.3f}"
        )
    
    return {
        "status": "success",
        "message": "Custom weights set successfully",
        "weights": weights.dict(),
        "total_weight": total_weight
    }

@app.post("/api/v3/documents/upload")
async def upload_document_v3(file: UploadFile = File(...), current_user: Dict[str, Any] = Depends(get_current_user_simple)):
    """Enhanced document upload for v3 with advanced extraction"""
    try:
        doc_id = len(document_storage) + 1
        
        # Read file content
        file_content = await file.read()
        
        logger.info(f"üöÄ V3 Upload: Processing {file.filename} ({len(file_content)} bytes)")
        
        # Advanced extraction using new methods
        extraction_data = ai_manager.extract_advanced_document_data(
            file_content, file.filename
        )
        
        # Store file info and content
        document_storage[doc_id] = {
            "id": doc_id,
            "filename": file.filename,
            "content_type": file.content_type,
            "status": "uploaded_v3",
            "size": len(file_content),
            "created_at": datetime.now().isoformat(),
            "version": "v3"
        }
        
        # Store advanced extracted content
        file_content_storage[doc_id] = {
            "raw_content": file_content,
            "extraction_data": extraction_data,
            "text_length": len(extraction_data["text"]),
            "tables_count": len(extraction_data["tables"]),
            "currencies_count": len(extraction_data["currencies"])
        }
        
        logger.info(f"‚úÖ V3 Document uploaded: {file.filename} (ID: {doc_id}, Tables: {len(extraction_data['tables'])}, Currencies: {len(extraction_data['currencies'])})")
        
        return {
            "id": doc_id,
            "filename": file.filename,
            "status": "uploaded_v3",
            "message": "Document uploaded successfully with advanced extraction",
            "extraction_summary": {
                "text_length": len(extraction_data["text"]),
                "tables_found": len(extraction_data["tables"]),
                "currencies_found": len(extraction_data["currencies"]),
                "structured_data_available": bool(extraction_data["structured_data"])
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå V3 Upload error: {e}")
        raise HTTPException(status_code=500, detail=f"Upload failed: {str(e)}")

@app.post("/api/v3/kp-analyzer/analyze")
async def analyze_v3(request: V3AnalysisRequest, current_user: Dict[str, Any] = Depends(get_current_user_simple)):
    """Expert KP analysis with 10-criteria system"""
    start_time = datetime.now()
    
    try:
        logger.info(f"üéØ Starting V3 analysis for documents: {request.document_ids}")
        
        # Validate documents exist
        for doc_id in request.document_ids:
            if doc_id not in document_storage:
                raise HTTPException(status_code=404, detail=f"Document {doc_id} not found")
            if doc_id not in file_content_storage:
                raise HTTPException(status_code=404, detail=f"Document content {doc_id} not found")
        
        # Get criteria weights
        if request.analysis_config and request.analysis_config.custom_weights:
            weights = request.analysis_config.custom_weights
        elif request.analysis_config and request.analysis_config.preset:
            preset_name = request.analysis_config.preset
            if preset_name not in criteria_weights_presets:
                raise HTTPException(status_code=400, detail=f"Unknown preset: {preset_name}")
            weights = criteria_weights_presets[preset_name]
        else:
            weights = criteria_weights_presets["balanced"]
        
        # Get TZ content if specified
        tz_content = None
        if request.tz_document_id:
            if request.tz_document_id not in file_content_storage:
                raise HTTPException(status_code=404, detail=f"TZ document {request.tz_document_id} not found")
            tz_data = file_content_storage[request.tz_document_id].get("extraction_data", {})
            tz_content = tz_data.get("text", "")
        
        # Combine all KP documents for analysis
        combined_content_data = {
            "text": "",
            "tables": [],
            "currencies": [],
            "structured_data": {},
            "metadata": {"document_ids": request.document_ids}
        }
        
        for doc_id in request.document_ids:
            content_data = file_content_storage[doc_id]["extraction_data"]
            combined_content_data["text"] += content_data["text"] + "\n\n"
            combined_content_data["tables"].extend(content_data["tables"])
            combined_content_data["currencies"].extend(content_data["currencies"])
            
            # Merge structured data
            for key, value in content_data["structured_data"].items():
                if key in combined_content_data["structured_data"]:
                    if isinstance(value, dict):
                        combined_content_data["structured_data"][key].update(value)
                    elif isinstance(value, list):
                        combined_content_data["structured_data"][key].extend(value)
                else:
                    combined_content_data["structured_data"][key] = value
        
        logger.info(f"üìä Combined analysis data: {len(combined_content_data['text'])} chars, {len(combined_content_data['tables'])} tables, {len(combined_content_data['currencies'])} currencies")
        
        # Perform AI analysis with v3 system
        try:
            logger.info("ü§ñ Calling Claude v3 analysis...")
            analysis_data = await ai_manager.analyze_with_claude_v3(
                combined_content_data, weights, tz_content
            )
            logger.info("‚úÖ Claude v3 analysis completed")
        except Exception as ai_error:
            logger.warning(f"‚ö†Ô∏è Claude v3 API failed: {ai_error}")
            logger.info("üîÑ Using enhanced fallback analysis...")
            analysis_data = ai_manager._generate_fallback_v3_analysis(combined_content_data, weights)
        
        # Calculate processing time
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Generate charts data if requested
        charts_data = None
        if request.generate_charts:
            charts_data = generate_analysis_charts(analysis_data, combined_content_data)
        
        # Create comprehensive v3 analysis result
        analysis_result = V3AnalysisResponse(
            id=request.document_ids[0],  # Primary document ID
            status="completed",
            overall_score=analysis_data.get("overall_score", 75),
            weighted_score=analysis_data.get("weighted_score", 75.0),
            company_name=analysis_data.get("company_name", "–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º–∞—è –∫–æ–º–ø–∞–Ω–∏—è"),
            summary=analysis_data.get("executive_summary", "–ü—Ä–æ–≤–µ–¥–µ–Ω —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–æ 10 –∫—Ä–∏—Ç–µ—Ä–∏—è–º"),
            executive_summary=analysis_data.get("executive_summary", ""),
            recommendations=analysis_data.get("recommendations", []),
            business_analysis=analysis_data.get("business_analysis", {}),
            criteria_weights=weights,
            risk_level=analysis_data.get("risk_level", "–£–º–µ—Ä–µ–Ω–Ω—ã–π —Ä–∏—Å–∫"),
            processing_time=processing_time,
            currency_data={
                "total_currencies": len(combined_content_data["currencies"]),
                "currencies": combined_content_data["currencies"][:10]  # First 10 currencies
            },
            extracted_tables=[
                {
                    "id": table.get("table_id", f"table_{i}"),
                    "rows": table.get("row_count", 0),
                    "columns": table.get("col_count", 0),
                    "preview": table.get("data", [])[:3] if table.get("data") else []  # First 3 rows
                }
                for i, table in enumerate(combined_content_data["tables"][:5])  # First 5 tables
            ],
            charts_data=charts_data,
            created_at=datetime.now().isoformat()
        ).dict()
        
        # Store v3 analysis result
        v3_analysis_storage[request.document_ids[0]] = analysis_result
        
        logger.info(f"üéØ V3 Analysis completed: Score {analysis_result['overall_score']}/100, Weighted: {analysis_result['weighted_score']:.1f}, Time: {processing_time:.2f}s")
        
        return analysis_result
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå V3 Analysis error: {e}")
        logger.error(f"‚ùå Traceback: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"V3 analysis failed: {str(e)}")

def generate_analysis_charts(analysis_data: Dict[str, Any], content_data: Dict[str, Any]) -> Dict[str, Any]:
    """Generate interactive charts for v3 analysis"""
    try:
        charts_data = {}
        
        # Criteria radar chart
        if "business_analysis" in analysis_data:
            criteria_names = []
            criteria_scores = []
            criteria_weights = []
            
            for criterion, data in analysis_data["business_analysis"].items():
                criteria_names.append(criterion.replace("_", " ").title())
                criteria_scores.append(data.get("score", 0))
                criteria_weights.append(data.get("weight", 0) * 100)  # Convert to percentage
            
            # Create radar chart data
            charts_data["criteria_radar"] = {
                "type": "radar",
                "data": {
                    "categories": criteria_names,
                    "scores": criteria_scores,
                    "weights": criteria_weights
                }
            }
            
            # Create bar chart for scores
            charts_data["criteria_bar"] = {
                "type": "bar", 
                "data": {
                    "categories": criteria_names,
                    "scores": criteria_scores,
                    "colors": [
                        "#22c55e" if score >= 85 else "#3b82f6" if score >= 70 else "#f59e0b" if score >= 55 else "#ef4444"
                        for score in criteria_scores
                    ]
                }
            }
        
        # Currency distribution pie chart
        if content_data.get("currencies"):
            currency_summary = {}
            for currency in content_data["currencies"]:
                curr_type = currency.get("currency", "unknown")
                if curr_type not in currency_summary:
                    currency_summary[curr_type] = {"count": 0, "total": 0}
                currency_summary[curr_type]["count"] += 1
                currency_summary[curr_type]["total"] += currency.get("amount", 0)
            
            charts_data["currency_pie"] = {
                "type": "pie",
                "data": {
                    "labels": list(currency_summary.keys()),
                    "values": [data["count"] for data in currency_summary.values()],
                    "amounts": [data["total"] for data in currency_summary.values()]
                }
            }
        
        return charts_data
        
    except Exception as e:
        logger.error(f"‚ùå Chart generation error: {e}")
        return {}

@app.get("/api/v3/analysis/{analysis_id}")
async def get_v3_analysis(analysis_id: int, current_user: Dict[str, Any] = Depends(get_current_user_simple)):
    """Get v3 analysis results by ID"""
    try:
        if analysis_id not in v3_analysis_storage:
            raise HTTPException(status_code=404, detail="V3 analysis not found")
        
        return v3_analysis_storage[analysis_id]
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Get V3 analysis error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v3/analysis/history")
async def get_v3_analysis_history(current_user: Dict[str, Any] = Depends(get_current_user_simple)):
    """Get v3 analysis history"""
    try:
        history = []
        for analysis_id, analysis_data in v3_analysis_storage.items():
            history.append({
                "id": analysis_id,
                "company_name": analysis_data.get("company_name", "Unknown"),
                "overall_score": analysis_data.get("overall_score", 0),
                "weighted_score": analysis_data.get("weighted_score", 0),
                "created_at": analysis_data.get("created_at", ""),
                "status": analysis_data.get("status", "unknown"),
                "risk_level": analysis_data.get("risk_level", "Unknown")
            })
        
        # Sort by creation date (newest first)
        history.sort(key=lambda x: x.get("created_at", ""), reverse=True)
        
        return {
            "history": history,
            "total": len(history),
            "version": "v3"
        }
        
    except Exception as e:
        logger.error(f"‚ùå Get V3 history error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v3/export/pdf/{analysis_id}")
async def export_v3_pdf(analysis_id: int, current_user: Dict[str, Any] = Depends(get_current_user_simple)):
    """Export v3 analysis as professional PDF with tender styling"""
    try:
        if analysis_id not in v3_analysis_storage:
            raise HTTPException(status_code=404, detail="V3 analysis not found")
        
        analysis_data = v3_analysis_storage[analysis_id]
        
        logger.info(f"üîÑ Generating V3 PDF for analysis {analysis_id}...")
        
        # Transform v3 data for tender PDF exporter
        tender_compatible_data = {
            "id": analysis_data["id"],
            "company_name": analysis_data.get("company_name", "Unknown Company"),
            "overall_score": analysis_data.get("overall_score", 0),
            "weighted_score": analysis_data.get("weighted_score", 0),
            "summary": analysis_data.get("executive_summary", ""),
            "analysis_type": "v3_expert",
            "processing_time": analysis_data.get("processing_time", 0),
            "created_at": analysis_data.get("created_at", datetime.now().isoformat()),
            "risk_level": analysis_data.get("risk_level", "Unknown"),
            "recommendations": analysis_data.get("recommendations", []),
            
            # Transform business analysis for PDF
            "business_analysis": {},
            "criteria_weights": analysis_data.get("criteria_weights", {}),
            "currency_data": analysis_data.get("currency_data", {}),
            "extracted_tables": analysis_data.get("extracted_tables", []),
            "charts_data": analysis_data.get("charts_data", {}),
            
            # V3 specific additions
            "version": "3.0",
            "analysis_method": "10-Criteria Expert Analysis",
            "total_criteria": 10
        }
        
        # Transform detailed business analysis
        if analysis_data.get("business_analysis"):
            for criterion, details in analysis_data["business_analysis"].items():
                tender_compatible_data["business_analysis"][criterion] = {
                    "score": details.get("score", 0),
                    "weight": details.get("weight", 0),
                    "details": details.get("details", ""),
                    "recommendations": details.get("recommendations", []),
                    "compliance_level": details.get("compliance_level", "medium"),
                    "risk_factors": details.get("risk_factors", [])
                }
        
        try:
            # Generate PDF using enhanced tender exporter
            pdf_buffer = tender_pdf_exporter.generate_kp_analysis_pdf(tender_compatible_data)
            
            # Generate filename
            company_name = analysis_data.get("company_name", "Unknown_Company").replace(" ", "_")
            pdf_filename = f"KP_Analysis_V3_{company_name}_{analysis_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
            
            # Save PDF to reports directory
            pdf_path = Path(f"data/reports/{pdf_filename}")
            with open(pdf_path, 'wb') as f:
                f.write(pdf_buffer.read())
            
            logger.info(f"‚úÖ V3 PDF generated successfully: {pdf_filename} (Size: {pdf_path.stat().st_size} bytes)")
            
            # Return PDF as streaming response
            pdf_buffer.seek(0)
            return StreamingResponse(
                BytesIO(pdf_buffer.read()),
                media_type="application/pdf",
                headers={"Content-Disposition": f"attachment; filename={pdf_filename}"}
            )
            
        except Exception as pdf_error:
            logger.error(f"‚ùå V3 PDF generation error: {pdf_error}")
            logger.error(f"‚ùå V3 PDF Traceback: {traceback.format_exc()}")
            
            # Enhanced fallback response with v3 details
            pdf_filename = f"DevAssist_Pro_KP_Analysis_V3_{analysis_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pdf"
            
            return {
                "message": "V3 PDF generation failed - enhanced fallback mode",
                "filename": pdf_filename,
                "analysis_id": analysis_id,
                "analysis_type": "v3_expert",
                "overall_score": analysis_data.get("overall_score", 0),
                "weighted_score": analysis_data.get("weighted_score", 0),
                "criteria_count": 10,
                "export_time": datetime.now().isoformat(),
                "error": str(pdf_error),
                "fallback_mode": True,
                "version": "3.0"
            }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå V3 PDF export error: {e}")
        raise HTTPException(status_code=500, detail=f"V3 PDF export failed: {str(e)}")

if __name__ == "__main__":
    print("Starting DevAssist Pro v3 - Enhanced AI-Powered Backend")
    print("=" * 60)
    print("Available endpoints:")
    print("   - Health Check:     http://localhost:8000/health")
    print("   - Register:         http://localhost:8000/api/auth/register")
    print("   - Login:            http://localhost:8000/api/auth/login")
    print("   - User Info:        http://localhost:8000/api/auth/me")
    print("   - Refresh Token:    http://localhost:8000/api/auth/refresh")
    print("   - V2 Upload:        http://localhost:8000/api/documents/upload")  
    print("   - V2 Analyze:       http://localhost:8000/api/documents/{id}/analyze")
    print("   - V2 Export PDF:    http://localhost:8000/api/documents/{id}/export-pdf")
    print("   - V3 Upload:        http://localhost:8000/api/v3/documents/upload")
    print("   - V3 Analyze:       http://localhost:8000/api/v3/kp-analyzer/analyze")
    print("   - V3 Export PDF:    http://localhost:8000/api/v3/export/pdf/{id}")
    print("   - V3 Weights:       http://localhost:8000/api/v3/criteria/weights/presets")
    print("   - V3 History:       http://localhost:8000/api/v3/analysis/history")
    print("   - Test Cyrillic:    http://localhost:8000/api/test/cyrillic-pdf")
    print("   - LLM Direct:       http://localhost:8000/api/llm/analyze")
    print("   - LLM Status:       http://localhost:8000/api/llm/providers")
    print("=" * 60)
    print("Features:")
    print("   * JWT Authentication System (Register, Login, Token Management)")
    print("   * KP Analyzer v2 - Basic Analysis")
    print("   * KP Analyzer v3 - Expert 10-Criteria Analysis (Protected)")
    print("   * Real Claude API Integration")
    print("   * Advanced PDF Processing with pdfplumber")
    print("   * Multi-currency Detection & Extraction")
    print("   * Interactive Charts Generation")
    print("   * Configurable Criteria Weights")
    print("   * Professional PDF Export with Cyrillic Support")
    print("   * Document Processing (PDF, DOCX, TXT)")
    print("   * Structured Data Extraction from Tables")
    print("   * Business Logic Analysis")
    print("   * Risk Assessment & Recommendations")
    print("   * Secure Password Hashing & Validation")
    print("=" * 60)
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info"
    )